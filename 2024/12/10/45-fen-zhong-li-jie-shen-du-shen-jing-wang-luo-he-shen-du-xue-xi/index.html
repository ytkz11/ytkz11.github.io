<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>Find everything you need | Find everything you need</title><meta name="author" content="ytkz"><meta name="copyright" content="ytkz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="Find everything you need"><meta name="application-name" content="Find everything you need"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="Find everything you need"><meta property="og:url" content="http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/index.html"><meta property="og:site_name" content="Find everything you need"><meta property="og:description" content="什么是深度学习？ - 45**分钟理解深度神经网络和深度学习** 刘利刚 中国科学技术大学图形与几何计算实验室 http:&amp;#x2F;&amp;#x2F;staff.ustc.edu.cn&amp;#x2F;~lgliu 【绪言】 近年来，人工智能（Artificial Intelligence, AI）和深度学习（Deep Learning,"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="ytkz"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="什么是深度学习？ - 45**分钟理解深度神经网络和深度学习** 刘利刚 中国科学技术大学图形与几何计算实验室 http:&amp;#x2F;&amp;#x2F;staff.ustc.edu.cn&amp;#x2F;~lgliu 【绪言】 近年来，人工智能（Artificial Intelligence, AI）和深度学习（Deep Learning,"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎回来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: ytkz","link":"链接: ","source":"来源: Find everything you need","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Find everything you need',
  title: 'Find everything you need',
  postAI: '',
  pageFillDescription: '什么是深度学习分钟理解深度神经网络和深度学习刘利刚中国科学技术大学图形与几何计算实验室绪言近年来人工智能和深度学习非常火爆在各个领域得到了广泛的应用在笔者所从事的计算机图形学领域也出现了越来越多的使用深度学习方法来解决各种问题的研究工作年月初笔者首次在第七届中国科学技术大学计算机图形学前沿暑期课程上讲授和分享了笔者从数学函数逼近论的角度来对基于深度神经网络的深度学习的进行理解之后不断有学生来向笔者进一步询问和交流有关资料和问题为了使学生们能够更好更快理解和使用深度神经网络和深度学习特撰写此文本文的目的是帮助非人工智能领域的学生主要是计算机图形学领域的学生及笔者的学生来搞懂深度学习这里指狭义的深度学习即基于的深度学习的基本概念和方法笔者尝试用通俗的语言从函数逼近论的角度来阐释深度神经网络的本质由于笔者的主要研究领域为计算机图形学而非人工智能领域因此本文仅仅为笔者从外行的角度对基于的深度学习的粗浅理解而非人工智能领域对和深度学习的权威解释因而笔者对其中的有些内容的理解是有限的甚至是有误的如有不当之处还请读者指正一从数据拟合说起在大学的数学分析微积分或者数值分析中大家都已学习并熟悉数据拟合问题及求解拟合问题的最小二乘法数据拟合问题在科学技术的各领域中我们所研究的事件一般都是有规律因果关系的即自变量集合与应变量集合之间存在的对应关系通常用映射来描述特殊情况实数集合到实数集合之间的映射称为函数这样能根据映射函数规律作出预测并用于实际应用有些函数关系可由理论分析推导得出不仅为进一步的分析研究工作提供理论基础也可以方便的解决实际工程问题比如适合于宏观低速物体的牛顿第二运动定律就是在实际观察和归纳中得出的普适性力学定律但是很多工程问题难以直接推导出变量之间的函数表达式或者即使能得出表达式公式也十分复杂不利于进一步的分析与计算这时可以通过诸如采样实验等方法获得若干离散的数据称为样本数据点然后根据这些数据希望能得到这些变量之间的函数关系这个过程称为数据拟合在数理统计中也称为回归分析这里值得提一下在实际应用中还有一类问题是输出的结果是离散型的比如识别图片里是人猫狗等标签的一种此时问题称为分类数据拟合类型我们先考虑最简单的情形即实数到实数的一元函数假设通过实验获得了个样本点我们希望求得反映这些样本点规律的一个函数关系如图所示如果要求函数严格通过每个样本点即则求解函数的问题称为插值问题一般地由于实验数据带有观测误差因此在大部分情况下我们只要求函数反映这些样本点的趋势即函数靠近样本点且误差在某种度量意义下最小称为逼近问题若记在某点的误差为且记误差向量为逼近问题就是要求向量的某种范数最小一般采用欧氏范数范数作为误差度量的标准比较容易计算即求如下极小化问题图数据拟合左输入的样本点中插值函数右逼近函数无论是插值问题还是逼近问题一个首要的问题就是函数的类型的选择和表示问题这是函数逼近论中的一个比较纠结的问题二函数逼近论简介函数的表示是函数逼近论中的基本问题在数学的理论研究和实际应用中经常遇到下类问题在选定的一类函数中寻找某个函数使它与已知函数或观测数据在一定意义下为最佳近似表示并求出用近似表示而产生的误差这就是函数逼近问题称为逼近函数或拟合函数在函数逼近问题中逼近函数的函数类可以有不同的选择即使函数类选定了在该类函数中确定的方式仍然是各式各样的对的近似程度误差也可以有各种不同的定义我们分别对这些问题进行理解和讨论逼近函数类在实际问题中首先要确定函数的具体形式这不单纯是数学问题还与所研究问题的运动规律及观测数据有关也与用户的经验有关一般地我们在某个较简单的函数类中去寻找我们所需要的函数这种函数类叫做逼近函数类逼近函数类可以有多种选择一般可以在不同的函数空间比如由一些基函数通过线性组合所张成的函数空间中进行选择如下是一些常用的函数类多项式函数类次代数多项式即由次数不大于的幂基的线性组合的多项式函数其中为实系数更常用的是由次基函数来表达的多项式形式称为多项式或多项式其中基函数三角多项式类阶三角多项式即由阶数不大于的三角函数基的线性组合的三角函数其中为实系数这些是常用的逼近函数类在逼近论中还有许多其他形式的逼近函数类比如由代数多项式的比构成的有理分式集有理逼近按照一定条件定义的样条函数集样条逼近径向基函数逼近由正交函数系的线性组合构成的维数固定的函数集等万能逼近定理在函数逼近论中如果一组函数成为一组基函数需要满足一些比较好的性质比如光滑性线性无关性权性所有基函数和为局部支集完备性正性凸性等其中完备性是指该组函数的线性组合是否能够以任意的误差和精度来逼近给定的函数即万能逼近性质对于多项式函数类我们有以下的万能逼近定理定理逼近定理对上的任意连续函数及任意给定的必存在次代数多项式使得逼近定理表明只要次数足够高次多项式就能以任何精度逼近给定的函数具体的构造方法有多项式或多项式等这里不详细展开类似地由分析理论或第二逼近定理只要阶数足够高阶三角函数就能以任何精度逼近给定的周期函数这些理论表明多项式函数类和三角函数类在函数空间是稠密的这就保障了用这些函数类来作为逼近函数是合理的逼近函数类选择的纠结在一个逼近问题中选择什么样的函数类作逼近函数类这要取决于被逼近函数本身的特点也和逼近问题的条件要求等因素有关在实际应用中这里其实存在着两个非常纠结的问题第一选择什么样的逼近函数类一般地需要用户对被逼近对象或样本数据有一些先验知识来决定选择具体的逼近函数类比如如果被逼近的函数具有周期性将三角函数作为逼近函数是个合理的选择如果被逼近的函数具有奇点将有理函数作为逼近函数更为合理等等第二即使确定了逼近函数类选择多高的次数或阶数比如如果选择了多项式函数类根据插值定理一定能找到一个次多项式来插值给定的个样本点但如果较大则这样得到的高次多项式很容易造成过拟合而如果选择的过小则得到的多项式容易造成欠拟合如图所示过拟合或欠拟合函数在实际应用中是没有用的因为它们的预测能力非常差图用不同次数的多项式拟合样本点蓝色点左欠拟合中合适的拟合右过拟合这里有个概念需要提及一下一个逼近函数表达能力体现在该函数的未知参数即公式中的系数与样本点个数的差也称为自由度如果逼近函数的未知参数越多则表达能力越强然而在实际的拟合问题中逼近函数的拟合能力并非越强越好因为如果只关注样本点处的拟合误差的话非常强的表达能力会使得样本点之外的函数值远远偏离期望的目标反而降低拟合函数的预测性能产生过拟合如图右所示人们发展出各种方法来减缓不能完全避免过拟合比如剔除样本点中的噪声数据去噪增加样本点数据量数据增广简化预测模型获取额外数据进行交叉验证或对目标函数进行适当的正则化等在此不详细叙述在实际应用中如何选择拟合函数的数学模型合适的逼近函数类及其阶数并不是一开始就能选好往往须通过分析确定若干模型后再经过实际计算比较和调整才能选到较好的模型需要不断的试验和调试称为调参过程是个需要丰富经验的技术活最小二乘法假设通过分析我们已经确定了逼近函数类及其次数记基函数一般线性无关为记为这些基函数所张成的线性空间函数空间则逼近函数可记为其中为待定权系数关于最小二乘法的一般提法是对给定的一组样本点数据要求在函数类中找一个函数使误差的模的平方即式达到最小对于分析极小化误差可得关于系数向量的法方程从而可求得由于法方程是一个线性方程组因此基于最小二乘法的函数求解也称为线性回归另外我们可在误差项中加个权表示不同点处的数据比重不同此时称为加权最小二乘方法另外还有移动最小二乘法等其他最小二乘法的改进方法此处不详细叙述三稀疏表达和稀疏学习在实际应用中节中所述的两个纠结问题时有发生人们发展出不同的方法来尝试解决岭回归当数据量较少的情况下最小二乘法线性回归容易出现过拟合的现象法方程的系数矩阵会出现奇异非满秩此时回归系数会变得很大无法求解这时在最小二乘法的结果式中加一个小扰动使原先无法求广义逆的情况变成可以求出其广义逆使得问题稳定并得以求解即事实上这个解对应于如下极小化问题的解其中参数称为正则化参数岭参数上述回归模型称为岭回归其与最小二乘法的区别在于多了关于参数的范数正则项这一项是对的各个元素的总体的平衡程度即限制这些权稀疏的方差不能太大实际应用中如果岭参数选取过大会把所有系数均最小化趋向于造成欠拟合如果岭参数选取过小会导致对过拟合问题解决不当因此岭参数的选取也是一个技术活需要不断调参对于某些情形也可以通过分析选择一个最佳的岭参数来保证回归的效果在此不详细叙述回归回归的极小化问题为其中正则项为范数正则项回归能够使得系数向量的一些元素变为稀疏因此得到的拟合函数为部分基函数的线性组合稀疏表达与稀疏学习根据回归的分析我们可通过对回归变量施加范数范数为元素中非元素的个数在很多时候可以用范数近似的正则项以达到对回归变量进行稀疏化即大部分回归变量为少数回归变量非这种优化称为稀疏优化也就是说对回归变量施加范数能够自动对基函数进行选择值为的系数所对应的基函数对最后的逼近无贡献这些非的基函数反映了的样本点集合的特征因此也称为特征选择通过这种方法为了保证防止丢失一些基函数特征我们往往可以多选取一些基函数甚至可以是线性相关的使得基函数的个数比输入向量的维数还要大称为超完备基或过冗余基在稀疏学习中亦称为字典然后通过对基函数的系数进行稀疏优化选择出合适非系数的基函数的组合来表达逼近函数这在一定程度上克服了节中所提出的两个纠结的问题因为这时可以选取较多的基函数及较高的次数通过稀疏优化来选择学习合适的基元函数也称为稀疏学习另外基函数字典和稀疏系数也可以同时通过优化学习得到称为字典学习对于矩阵形式比如多元函数或图像矩阵的稀疏表现为矩阵的低秩近似为核模很小则对应着矩阵低秩求解问题在此不详细叙述稀疏学习与最近十年来流行的压缩感知理论与方法非常相关也是机器学习领域的一种重要方法其深厚的理论基础由华裔数学家陶哲轩年国际数学家大会菲尔兹奖得主斯坦福大学统计学教授年国际数学家大会高斯奖得主等人所建立已成功用于信号处理图像与视频处理语音处理等领域在此不详细展开介绍近年来笔者及其团队成功地将稀疏学习的理论和方法推广用于三维几何建模与处理中在计算机图形学领域已发表十余篇相关论文包括在顶级期刊上发表了篇论文有兴趣的读者可详细查看笔者的相关论文三维几何处理的稀疏优化方法的综述论文基于基函数稀疏选择的曲面生成基于字典学习的曲面重建基于稀疏优化的局部重心坐标基于特征稀疏选择的三维形状共同分割基于矩阵低秩优化的三维形状正向计算图稀疏学习方法用于三维几何的建模与处理笔者的工作四高维的逼近函数上面讨论了映射为一元函数的情形上述的有关概念和方法同样可以推广至多元函数及向量值函数的情形如图图不同情形维数的逼近函数左上一元函数右上多元函数左下一元向量值函数右下多元向量值函数多元函数假设有多元函数即的一组测量样本数据设中的一组基函数为要求函数使得误差的平方和最小这与式和节的极值问题完全一样系数同样通过求解法方程得到上述过程称为多元函数的最小二乘拟合在函数论中高维空间中的基函数的构造也有许多方法为了简便在许多时候我们采用张量积形式的基函数即每个基函数是各个维数的一元基函数的乘积形式在计算机图形学和计算机辅助几何设计领域经常采用向量值函数如果因变量有多个即称为向量值函数比如空间螺旋线的参数方程就是一个从一维到三维的向量值函数如果自变量的维数比应变量的维数低则自变量可看作为应变量的参数在式中变量可看作为该螺旋线的参数对于向量值函数可以看成是多个一元函数或多元函数一般共享基函数即可所有处理方法包括最小二乘法都是对每个函数分别操作的与前面单个函数的处理方法完全一样如果用数学方式来表达前面的很多符号由向量变为矩阵的形式而已详细可查看数学分析五参数曲线曲面拟合前面讨论的函数所能表达的曲线比较简单无法表达封闭和多值的曲线因此在计算机图形学和计算机辅助几何设计中常用参数曲线曲面本质是向量值函数见节来表达几何形状用于几何造型另外在计算机图形学中还有一种利用隐函数来表达曲线的方式也常用于造型中这里不展开介绍二维参数曲线从数学来看一条曲线的本征维度是一维的不论是从哪个维度的空间来看从直观来看该曲线能被拉成为一条直线或线段即一条有端点的曲线能和一条线段建立对应如图左所示因此嵌入到二维空间中的曲线可由单个参数来表达即有如下形式的参数形式其中为该曲线的参数均为的函数分别表示曲线上对应于参数的点的两个坐标分量一般参数曲线表达为本质上这是一个从到的向量值函数三维参数曲面类似地一张二维流形曲面的本征维度是二维的一张曲面能与平面上的一块区域建立对应如图右所示因此嵌入到三维空间中的曲面可由两个参数来表达即有如下形式的参数形式其中为该曲面的参数均为的二元函数分别表示曲面上对应于参数的点的三个坐标分量一般参数曲面表达为本质上这是一个从到的向量值函数图参数曲线曲面左单参数曲线右双参数曲面参数曲线曲面拟合下面我们介绍如何使用参数曲线来拟合平面上的一组有序样本点参数曲线拟合的问题描述如下给定平面上一组有序样本点我们希望寻求一条拟合这些点的参数曲线关系如需决定一条拟合曲线首先需要确定每个样本点所对应的参数对于一组有序的样本点所确定的一种参数分割称之为这组样本点的参数化参数化的本质就是找一组恰当的参数来匹配这一组样本点以便使这条拟合出来的曲线美观合理在参数曲线拟合中样本点的参数化是个非常基本而重要的问题不同的参数化对拟合曲线的形状会产生很大的影响图显示了三种参数化均匀参数化弦长参数化和中心参数化对拟合曲线形状的影响图不同的参数化导致不同的参数曲线拟合结果类似地如果利用参数曲面来拟合空间中给定的一组样本点首先也需要确定这些样本点在平面上的参数化如图所示在曲面参数化中不仅需要建立样本点及其参数的一一对应我们还希望参数区域的网格保持原始网格的一些重要的几何特性比如角度边长等图三维曲面的参数化曲面参数化是计算机图形学中非常基础且重要的一个研究课题也一直是研究热点之一笔者及所在的课题组也在这个课题方面作了大量的工作有兴趣的读者可详细查看笔者的相关论文平面参数化球面参数化流形参数化六从人工神经网络的观点来看拟合函数为了讲清楚人工神经网络下面我们先从神经网络的观点来看传统的拟合函数和曲线曲面一元函数对于一元的逼近函数可以看成为如图的三层神经网络其中只有一个隐层隐层上有个节点激活函数分别为基函数从这里我们将基函数称为激活函数输入层到隐层的权设为常值图左也可以看成为将输入层的值直接代入不做任何变换用虚线表示到激活函数图右隐层到输出层的权为基函数的组合系数中间隐层的输出节点所形成的向量可看作为输入量的特征现在我们开始用机器学习的语言来描述数据拟合的过程整个神经网络就是由中间节点的激活函数及权系数所决定的一个函数我们称样本点为训练数据称函数在训练数据上的误差度量为损失函数通过训练数据来极小化损失函数得到权系数的过程称为训练或学习如果损失函数取为的形式则网络的训练过程本质上就是前面所讲的最小二乘法节图一元逼近函数的神经网络左输入层到隐层的权系数均为常值右输入层到隐层看成为直接代入用虚线表示多元函数和向量值函数考虑一般的逼近函数设中的一组基函数为则函数可看成为如下图的一个三层的神经网络注意这里隐层的激活函数都是维函数从输入层到隐层也是直接代入输出层的各个分量共享隐层的激活函数图多元函数和向量值函数的神经网络从输入层到隐层是变量直接代入到激活函数拟合曲线曲面拟合曲线可以看作为一个多层的神经网络如下图所示输入层样本点到参数化是一个变换可看成为一个神经网络参数化到输出层是一个变换单变量基函数组合也是一个神经网络输出层的两个分量共享基函数隐层的激活函数为双变量基函数隐层的激活函数为单变量基函数中间隐层隐层的输出节点所形成的向量均可分别看作为输入量在不同维数空间的特征图拟合曲线的神经网络输入层样本点到参数化是一个神经网络变换参数化到输出层是一个神经网络变换类似地拟合曲面也可以看作为一个如下图的多层神经网络图拟合曲面的神经网络从曲线曲面拟合的过程及神经网络结构来看如果将整个网络进行优化则拟合的过程是寻求样本点的参数化以及表达函数的基函数使得拟合误差极小化在这里我们是知道给定样本点的本征维数因此人为设置了第一个隐层的维数为曲线或曲面然而在实际应用中我们并不知道数据的本征维数此时需要利用其他方法比如流形学习或降维方法来检测或猜测隐层的维数或者直接根据经验来不断调参和测试需要提及的是在计算机图形学中传统的大部分方法是将参数化的问题和曲线曲面拟合问题分为两个问题来解决有些工作专门做参数化有些工作专门研究拟合问题其中参数化的工作更为重要直接决定了拟合曲线曲面的质量此处不详述需要注意的是本节只是从神经网络的观点来看曲线曲面的参数化和拟合的问题在计算机图形学中我们不是简单的通过上述网络来求参数化单个样本点而是要考虑更多的样本点一起来求解参数化这个后面还会详细介绍七通用人工神经网络有了上述的理解我们现在来理解人工神经网络就非常容易了函数拟合回顾回顾前面所述的函数逼近论中的函数拟合问题始终是存在着两个纠结一是选择什么样的逼近函数类即选择什么类型的基函数二是选择多高的次数或阶数另一方面如果从图或图的神经网络的观点来看网络的结构设置都存在着如下两个纠结隐层中的节点中使用什么样的激活函数基函数隐层中设置多少个节点基函数的次数在传统逼近论中上述两个问题就没有好方法来解决第一个问题针对具体问题可预先设定是通过不断试错来看结果的好坏来决定的这是因为虽然有些基函数的性质很好但是次数或阶数过高比如多项式基或三角函数基就会产生震荡也容易产生过拟合使得拟合函数的性态不好因此一般不会采用次数或结束太高的基函数为此人们发展了分段拟合方法即将数据分为若干段每段分别拟合各段的拟合函数保证一定的光滑性称为样条函数在逼近论和计算机图形学领域人们发展出了很多漂亮的样条的理论与方法对于参数类型的高维曲线曲面还存在着参数化的问题这时还存在另一个纠结就是要设置多少个隐层图和图使用简单元函数作为激活函数为了克服上述两个纠结的问题是否可以通过其他形式来生成基函数注意到对于任意一个非常值的一元函数这里我们称为元函数其沿着方向的平移函数以及沿着方向的伸缩函数都与原函数线性无关也就是说如果能有足够多的变换所生成的函数其线性组合所张成的函数空间就能有充分的表达能力那么这些函数可能都线性无关是否能构成逼近一般函数的基函数呢一个自然的想法就是我们能否以这个作为激活函数让网络自动地去学习这些激活函数的变换或者说去学习一些基函数来表达所需要的拟合函数呢这就是图所示的神经元变量乘以一个伸缩加上一个平移称为偏置即变量的仿射变换成为神经元的输入然后通过激活函数复合后成为该神经元的输出图一元单变量函数的神经元结构对于多变量的情形多元函数神经元的结构如图所示变量的线性组合加上一个平移称为偏置即变量的仿射变换成为神经元的输入然后通过激活函数复合后成为该神经元的输出图多元多变量函数的神经元结构不失一般性我们下面就直接以多元函数的形式来介绍一般神经网络的结构对于向量值函数是多维的则分别将的各个维数分别看作一个多元函数处理而已这些函数共享节点基函数即可单隐层神经网络一个多元函数的神经网络的结构如图所示有一个输入层一个隐层及一个输出层输入层除了变量外还有一个常数节点隐层包含多个节点每个节点的激活函数都是隐层的输出就是输入层节点的线性组合加偏置即仿射变换代入到激活函数的复合函数输出层是这些复合函数的组合这个网络的所有权系数输入层与隐层之间的权及偏置项及隐层与输出层之间的权此层一般不用偏置项作为这个神经网络的参数变量需要通过极小化损失函数来求解的这个过程称为训练或学习与前面所述的逼近论中的拟合函数类似网络的隐层的节点数是需要通过不断调试和尝试来确定的隐层的节点输出称为输入数据的特征图多元多变量函数的单隐层神经网络结构整个网络的学习过程本质上就是在学习所有的系数参数最后得到的拟合函数为一些函数的线性组合表达这些组合函数实质上就是表达函数的基函数从函数逼近论的角度我们可以这样来理解神经网络神经网络的学习过程本质上是在学习基函数这些基函数是通过激活函数通过平移和伸缩变量的仿射变换来得到的当然天下没有免费的午餐以前函数逼近论无法解决的两个纠结这里仍然存在第一个纠结是如何选取基函数这里使用激活函数然后学习其变换来得到第二个纠结是选择多少个基函数这里为隐层的神经元个数从这个观点来看神经网络本质上就是传统的逼近论中的逼近函数的一种推广它不是通过指定的理论完备的基函数来表达函数的而是通过简单的基元函数激活函数的不断变换得到的基函数来表达函数的实质上是二层复合函数此时我们当然要问个问题将函数经过充分多的平移和伸缩包括它们的组合所线性张成的函数空间其表达能力有多强这个函数空间是否在所有函数空间是稠密的如果问题是正确的那么就是说对于任何一个给定的函数总能找到函数的多次平移和缩放的函数其线性组合能够逼近给定的这个函数也就是说图中的神经网络只要隐层的节点数足够多该网络所表达的函数就能逼近任意的函数幸运的是上述猜想在很多情况下是成立的有以下的万能逼近定理所保证万能逼近定理记为空间中的单位立方体我们在这个定义域中来描述万能逼近定理记为上的连续函数空间为上的可测函数空间为上相对测度的可积函数空间即设给定一元激活函数首先给出如下定义定义称函数为压缩函数如果单调不减且满足定义称函数为可分辨的若对于有限测度由可得到定义记为所有由激活函数变换及线性累加所构成的维函数空间即具有个节点的单隐层神经网络所表达的维函数定理若是压缩函数则在中一致稠密在中按如下距离下稠密定理若是可分辨的则在中按连续函数距离下稠密定理若是连续有界的非常值函数则在中稠密定理若是无界的非常值函数则在中稠密上述定理这里就不详细解释稍微有些实分析和泛函分析的基础就能看懂用通俗的话来说就是对任意给定的一个中的函数只要项数足够多中就存在一个函数使得在一定精度下逼近也就是说图所表达的单隐层的神经网络所表达的维函数能够逼近中的任意一个函数根据上述定理可知函数只要满足一定条件即可作为一个合适的激活函数但还是存在着一个巨大的纠结就是隐层中的节点数量该取多大仍是未知的在实际中这个是要不断通过测试和调试来决定的这个过程称为调参定理的详细证明可见以下论文深度神经网络多隐层神经网络将上述的神经网络的隐层进一步推广到多层即可得到多隐层的神经网络即深度神经网络深度的意思就是多层其网络结构如图所示由于每相邻的两层的节点都是全部连接的因此在机器学习领域也叫做全连接神经网络或多层感知机图只是显示了的函数的神经网络的结构对于一般的的向量值函数只要将最后输出层多几个节点即可即每个维度分量都是一个的函数共享了前面的所有网络层结构每个隐层的输出都可以看成输入数据在不同维数空间下的特征注意不要在输出层使用激活函数因为最后的输出就是那些基函数的线性组合表达即可从数学形式上看深度神经网络就是一个多层复合函数可以证明表示定理任意一个多元函数可以表示成若干个单变量函数的复合事实上只需要三层网络即可这为使用深度神经网络用来逼近任意高维函数提供了理论基础但遗憾的是该定理只证明了复合函数逼近任意函数的存在性但具体使用什么样的函数未可知图多元多变量函数的深度神经网络结构因此深度神经网络也能很强地表达中的任意的函数但在具体应用中选用多少个隐层每一隐层选用多少节点这些都是需要根据损失函数的进行不断调试的这个过程也称为调参另外从这个观点可以容易理解各种深度神经网络的工作机制包括一般深度神经网络卷积神经网络神经网络等八深入理解深度神经网络激活函数由节中的万能逼近定理可知激活函数的选择可以很多种常用的激活函数如图所示图常用的激活函数在早期函数和函数是人们经常使用的激活函数近年来随着神经网络的隐层不断增加后人们偏向于使用函数作为激活函数这是因为函数的在轴正向的导数是不容易产生梯度消失问题见节这里值得一提的是如果使用函数作为激活函数则最后生成的拟合函数为分片线性函数关于激活函数的理解和讨论机器学习领域及互联网上有很多文章进行过解释都是解释为激活函数的主要作用是提供网络的非线性建模能力提供了分层的非线性映射学习能力等等我们从逼近论的角度将非线性激活函数理解为拟合函数的基函数生成器如果激活函数为线性的则无法生成表达非线性函数的基训练神经网络就是在学习这些基函数这是一种全新的观点也是非常容易理解的反向传播算法优化算法从数学上看深度神经网络就是一个多层复合函数在机器学习中这个复合函数的好坏依赖于对目标函数最大化或者最小化的过程我们常常把最小化的目标函数称为损失函数它主要用于衡量机器学习模型此处为深度神经网络的预测能力常见的损失函数有均方误差误差平均绝对误差误差分位数损失交叉熵对比损失等损失函数的选取依赖于参数的数量异常值机器学习算法梯度下降的效率导数求取的难易和预测的置信度等若干方面这里就不展开讨论给定了一批训练数据计算神经网络的问题就是通过极小化损失函数来找到网络中的所有权值参数包括偏置参数这个过程本质上就是利用这个神经网络函数来拟合这些数据这个过程也称为训练或学习误差反向传播算法是神经网络训练时采用的一种通用方法本质是随机梯度下降法最早见于如下论文由于整个网络是一个复合函数因此训练的过程就是不断地应用复合函数求导链式法则及梯度下降法反向传播算法从神经网络的输出层开始利用递推公式根据后一层的误差计算本层的误差通过误差计算本层参数的梯度值然后将差项传播到前一层然后根据这些误差来更新这些系数参数值得一提的是梯度下降法并不是唯一的优化方法也有不少人使用其他优化方法比如方法来作为优化器优化深度神经网络的求解什么是好的激活函数由万能逼近定理可知只要网络规模设计得当使用非线性函数作为激活函数比如节中常用的激活函数的逼近能力都能够得到保证然后由上一节的反向传播算法可知算法过程中计算误差项时每一层都要乘以本层激活函数的导数因此会发生很多次的导数连乘如果激活函数的导数的绝对值小于多次连乘之后误差项很快会衰减到接近于而参数的梯度值由误差项计算得到从而导致前面层的权重梯度接近于参数不能得到有效更新这称为梯度消失问题与之相反如果激活函数导数的绝对值大于多次连乘后权重值会趋向于非常大的数这称为梯度爆炸长期以来这两个问题一直困扰神经网络层次无法变得很深最近几年函数被经常使用作为深度神经网络的激活函数有两个主要理由该函数的导数为忽略在处不可导的情形计算简单在正半轴导数为有效的缓解了梯度消失问题虽然它是一个分段线性函数但它具有非线性逼近能力使用激活函数的深度网络的本质是用分段分片线性函数超平面去逼近目标后来人们在的基础上又改进得到各种其他形式的激活函数包括等多个隐层的几何意义现在我们已经理解了深度神经网络的本质就是一个复杂的多层复合函数训练的过程就是在求激活函数变换得到的基函数因此训练神经网络的本质就是在学习基函数这跟稀疏学习中的字典学习非常类似殊途同归我们来看一下深度神经网络所表达的拟合函数与传统所用的拟合函数节的类比与区别传统的拟合函数是在某一函数类由某些具有良好性质的基函数所张成的函数空间中进行选择基函数是预先给定的个数也是预先设定的或者稀疏选择的系统的求解变量是基函数组合系数神经网络所表达的拟合函数是由某一激活函数对变量的平移和伸缩变换而来个数是预先设定的神经元个数系统的求解变量是基函数的变换因此可以看成是在学习基函数传统的拟合函数也是一种神经网络见第节因此也可以拓展成深度网络复合函数但问题是次数或阶数经过复合后会变得非常高更易造成过拟合多层神经网络中的不同层所用的激活函数也可以使用传统的基函数因此还可推广为更一般的深度神经网络那么为何要用多个隐层呢事实上通过节的内容就不难理解了每个隐层就是将前一层的数据作为输入当然也包括输入层数据在当前这个维数的空间中的一种映射本质上就是在做参数化如果当前层的维数小于上一层的维数就相当于将上一层的数据参数化到低维空间中当然也可以参数化到高维空间以提高自由度在节中对于三维曲面我们已知它是二维流形因此我们知道将其映射到二维作为参数域就很自然了因此有了图这样的网络结构但对于一般的数据比如人脸数据是在背景空间上观测的可能数据的维数非常高比如的图像按照像素个数的维数达到万维但这些数据并不会充满整个高维空间会具有低维的结构称为本征维数即数据落在一个低维子空间或低维流形上并呈现一定的概率分布从数据的角度来看一个嵌入到高维空间的低维流形本质上就是一个参数曲面参数域是那个低维空间因此将高维数据参数化到低维的空间是可行的这里也出现了两个纠结的问题第一对于一个实际数据拟合问题应该设置多少个隐层第二每个隐层应该设置多少个节点该隐层的维数在机器学习领域隐层的层数称为网络的深度隐层的维数成为网络的宽度对于基于深度神经网络的学习到底是越深越好还是越宽越好从数学上来看网络越深表示一个复杂的函数可以经过多次参数化的变换映射到最终的目标这个是合理的举个例子比如我们想将一个三维网格曲面参数化到一个平面如果这个输入的曲面比较复杂开口参数化边界很小则要将曲面一次性展开到的映射比较困难此时可以多进行几次从到的映射将曲面先在慢慢张开每次张开的映射比较容易表达和计算最后再映射到上就相对容易了当然还可进行多次从到的映射生成更好的参数化注意其中任何一个映射都可以是用一个神经网络来表达图带有特征的曲面的拟合原始输入三维数据为的简单参数化参数化为优化过后的参数化最后的拟合曲面在计算机图形学中我们针对曲面展开及拟合问题在如下的工作中用到了这种思想如图所示为一个带有尖锐特征的输入点云如果将其简单参数化为在其上构建的拟合曲面很难表现曲面的尖锐特征这时我们对中的参数化再做一个映射优化为这时在其上做的拟合曲面则能很好地表现出尖锐特征注意这里生成拟合曲面的基函数中我们采用了非光滑基函数采用非光滑基函数来表现曲面特征的思想首次在我们的如下论文中所采用因此上述工作本质上是一个使用层神经网络个隐层来优化拟合曲面这里顺便提一下在计算机图形学中最近一些年有不少工作在做曲面参数化的优化工作比如满足无翻转低扭曲等约束从机器学习的观点来看就是在做更多的参数映射的优化计算当然这些工作并不能简单地看成一个深度神经网络来做这是因为对参数化的问题我们是知道曲面所有数据的整体结构的而不只是一些离散的采样点因此我们在做参数化优化的时候我们可以充分利用曲面的整体结构来得到一些几何度量来满足各种约束而这些是神经网络优化所做不到的总之对于第一个问题隐层越多表示网络越深确实会使得最后的映射带来更好的表现其数学本质是将输入数据不断在不同的空间中进行参数化映射最后得到的映射可能非常复杂的映射表达为若干个相对简单映射的复合对于多隐层的解释和讨论机器学习领域及网上有很多文章进行过解释大部分都解释为提取数据的特征等等我们从逼近论的角度结合三维曲面拟合的例子将多隐层优化看成数据在不同维数空间的参数化是非常容易直观理解的对第二个问题每个隐层的具体的节点数或维数该如何设置呢对于一般的数据这个就很难直观确定了一般要根据经验当然也有一些办法来进行指导我们将在下一节中理解流形学习数据降维简介对于一般的数据我们并不知道其本征维数即参数域空间那么映射参数化到多少维空间是合适的呢事实上这个也是个非常纠结的问题这个问题就是寻求高维数据的本征维数在机器学习领域称为流形学习或降维问题如图所示数据是在三维空间给出的每个点是个坐标看起来是三维数据但其本质上位于三维空间的一张曲面上即二维流形曲面上其本征维数是因此可以将其一一映射到平面上图流行学习图片来自论文提到流形学习不能不提到如下的两篇开山之作同时发在的文章流形学习在本世纪初的前一个十年是一个研究热点由上面两篇论文带起来的至今已发展出了许多方法从简单的线性方法比如到后面的非线性方法比如局部线性嵌入等距映射法特征映射局部保投影法等这里不详细介绍需要指出的是正如第节所讲的三维曲面参数化的计算我们计算数据的本征维数的时候是需要利用所有数据的信息及其几何或拓扑结构一起来分析的而不能像神经网络训练那样仅仅将数据灌到神经网络中去更新权系数端对端学习在传统的机器学习中很多任务的输入数据的维数都非常大不易直接处理往往需要先降到合适的维数然后再选择合适的预测模型来做拟合传统机器学习的流程一般由两个主要的模块组成第一个模块称为特征工程将原始输入数据高维向量变换为一个低维向量即用这个低维向量来作为表达输入数据的特征或描述子不同的人有不同的方法来计算这个低维特征称为特征抽取如图显示了不同的图像特征抽取方法各种计算的特征都有一定的实用范围因此有人就提出如何选择或组合各种特征得到更好的特征这是特征工程的另一个子问题称为特征选择针对三维形状分割我们发现对三维局部块的众多描述子并不是串联起来一起使用更好而是针对不同的形状的部位使用不同的描述子会更好于是我们使用稀疏选择的方法来选择形状描述子用于形状分割详细可见如下论文图图像的各种特征抽取方法由于这种特征是通过人为设计的算法来得到的因此这种特征称为人工特征人工特征的抽取需要人们对输入数据的认知或者领域知识因此在很多情况下会局限于人的经验和认知第二个模块称为预测模型即选用什么样的预测方法来拟合给定的数据我们前面所讲的拟合就是指这个过程预测模型的目标就是希望学习到的模型在预测未知目标时越精确越好一般要考虑模型的复杂度及精确度等因素在机器学习领域有非常多的方法这里不一一介绍上述两个模块中的每个模块都是一个独立的任务其结果的好坏会影响到下一步骤从而影响整个训练的结果这是非端到端的如图上所示特征提取的好坏异常关键甚至比预测模型还重要举个例子对一系列人的数据分类分类结果是性别男或女如果你提取的特征是头发的颜色无论分类算法如何分类效果都不会好如果你提取的特征是头发的长短这个特征就会好很多但是还是会有错误如果你提取了一个超强特征比如染色体的数据那你的分类基本就不会错了这就意味着特征需要足够的经验去设计这在数据量越来越大的情况下也越来越困难于是就出现了端到端的网络特征可以自己去学习所以特征提取这一步也就融入到算法当中不需要人来干预了即不需要将任务分为多个步骤分步去解决而是从输入端的数据直接得到输出端的结果前面介绍的深度神经网络就是一个端到端的学习方法如图下所示端到端学习的好处在于使学习模型从原始输入到最终输出直接让数据说话不需人工设计的模块给模型更多可以根据数据自动调节的空间增加模型的整体契合度图非端到端的学习上与端到端的学习下但端到端学习也有不足第一端到端的深度神经网络需要大量的样本数据才能工作得很好因此很多公司都在花费大量的人力物力去生成标注数据一般通过人工标注催生了很多专门做数据标注的外包公司第二人工设计的特征并不是都不能用的有些人工特征还是能代表着人类智能的是可以用来作为合理的特征的而端到端的学习无法用这些特征当然通过适当的改造和结合也可以集成人工特征到深度神经网络中这里就不展开讨论深度学习成功的原因基于深度神经网络的端到端学习在最近几年取得很大的成功被大量应用与计算机视觉语音识别自然语音处理医学图像处理等领域中从数学本质上来看神经网络就是一个映射函数而已在上世纪年代就有了感知机但为什么以前没有火呢主要有以下的两方面的原因第一之前没有那么大规模的数据量第二以前的工程技术先进优化算法及支持大规模并行计算的硬件无法求解的很深的神经网络因此在上个世纪年代各种浅层模型大行其道比如只有一层隐层的支撑向量机以及没有隐层的逻辑回归方法等直到最近几年随着各种传感器的大量使用以及移动互联网的广泛应用能够获取和产生海量级甚至级的数据比如文本图像语音等特别是图像数据库的诞生以及基于该数据库的各种竞赛直接将计算机视觉领域中的深度学习的研究与应用推向了落地催生了大量的计算机视觉的人工智能公司过拟合自从深度学习火了后各种深度神经网络或网络随之产生了比如年的有层年的有层年的有层年的有层甚至还出现了上千层的深度神经网络从数学上可知拟合函数所带的参数的个数与样本数据的个数之间的差代表着这个拟合函数的自由度网络越来越深后拟合模型中的可调整参数的数量就非常大通常能达到百万计甚至几十亿级因此层数很大的深度网络模型过于复杂能够表达一个自由度非常大的函数空间甚至远高于目标函数空间过完备空间即自由度远大于这样就很容易导致过拟合如前面所讨论过的图右所示过拟合可以使得拟合模型能够插值所有样本数据拟合误差为但拟合误差为不代表模型就是好的因为模型只在训练集上表现好由于模型拟合了训练样本数据中的噪声使得它在测试集上表现可能不好泛化性能差为此人们采取了不同的方法来缓解过拟合无法完全避免比如正则化数据增广网络剪枝等在使用深度神经网络来做深度学习的应用中很多工作都是直接使用现有的深度神经网络或者改造现有的深度神经网络如果网络设计不够好大部分结果都可能有过拟合的现象在实际应用中我们可以挑选样本数据中的一部分作为训练数据集其他的作为测试数据集如果网络在训练数据集上表现好即损失函数很小而在测试数据集上表现不够好那么就可能发生了过拟合此时就适当减少网络的层数或者节点数这个过程是需要有耐心可能需要花费很长时间才能调到一个合适的网络需要靠感觉经验和运气因此有人戏称调试深度神经网络的过程为炼丹类比神经科学的解释我们已从函数逼近论的角度来理解了人工神经网络本质是传统逼近函数的一个推广由人工选取基函数到自动学习基函数由人工提取特征到自动计算特征参数化能够较容易理解人工神经网络学习是如何工作的在机器学习领域人们从生物神经网络的角度来解释和理解人工神经网络的工作原理将人工神经网络看成为一个可以模拟人脑工作行为的函数在生物神经科学包括脑科学认知神经科学等的研究中人们发现人体的神经元通过树突接受信号这些信息或信号随后被传递到脑细胞或细胞体在细胞体内部所有的信息将被加工生成一个输出当该输出结果达到某一阈值时神经元就会兴奋并通过轴突传递信息然后通过突触传递到其他相连的神经元神经元间传输的信号量取决于连接的强度如图左我们来类比人工神经网络如果把树突想象成人工智能网络中基于突触感知器的被赋予了权重的输入然后该输入在人工智能网络的细胞体神经元中相加如果得出的输出值大于阈值单元那么神经元就会兴奋激活并将输出传递到其它神经元如图右图左人脑神经元结构右从生物神经网络类比得出的人工神经元结构图片来自互联网在计算机视觉领域人们也从视觉神经科学中类比来解释了在计算机视觉领域用得最多的卷积神经网络的工作机理图视觉皮层结构图片来自互联网由视觉机理的研究发现动物大脑的视觉皮层具有分层结构眼睛将看到的景象成像在视网膜上视网膜把光学信号转换成电信号传递到大脑的视觉皮层视觉皮层是大脑中负责处理视觉信号的部分年有科学家进行了一次实验他们在猫的大脑初级视觉皮层内插入电极在猫的眼前展示各种形状空间位置角度的光带然后测量猫大脑神经元放出的电信号实验发现当光带处于某一位置和角度时电信号最为强烈不同的神经元对各种空间位置和方向偏好不同这一成果后来让他们获得了诺贝尔奖目前已经证明视觉皮层具有层次结构从视网膜传来的信号首先到达初级视觉皮层即皮层皮层简单神经元对一些细节特定方向的图像信号敏感皮层处理之后将信号传导到皮层皮层将边缘和轮廓信息表示成简单形状然后由皮层中的神经元进行处理它颜色信息敏感复杂物体最终在皮层被表示出来卷积神经网络可以看成是上面这种机制的简单模仿它由多个卷积层构成每个卷积层包含多个卷积核用这些卷积核从左向右从上往下依次扫描整个图像得到称为特征图的输出数据网络前面的卷积层捕捉图像局部细节信息有小的感受野即输出图像的每个像素只利用输入图像很小的一个范围后面的卷积层感受野逐层加大用于捕获图像更复杂更抽象的信息经过多个卷积层的运算最后得到图像在各个不同尺度的抽象表示由底层到高层的特征表达如图所示图基于卷积神经网络的深度学习所得到的人脸图像的分层特征图片来自于大学的论文笔者认为利用神经科学的神经元或神经网络来解释深度学习中的人工神经网络这仅仅是类比而已从我们的分析可知深度学习中的人工神经网络并没有真正的认知和学习如果我们人类对自己的认知系统无法完全了解的话我们是很难发展出真正的人工智能技术深度人工神经网络的局限保持结构的映射我们从数学上理解了深度人工神经网络本质上是表达不同维数空间之间的一个映射函数给了一些样本点后通过调整网络的参数权系数不断极小化损失函数从而达到最好的拟合结果得到了拟合函数这个拟合过程只极小化了逐点的误差或者累积逐点误差而无法考虑点与周围点之间的其他信息比如邻域几何结构局部几何度量流形的整体拓扑结构等如果要考虑点之间的几何与拓扑的结构信息则深度人工神经网络就无能为力当然一定要改造也是可以的这时就需要其他的数学方法了有点像数学分析中逐点收敛和一致收敛逐点连续和一致连续的区别例如前面介绍的流形学习中方法保持每个样本点与它相邻的多个点的线性组合体现了局部线性来重构低维空间的点的分布相当于用分段的线性面片近似代替复杂的几何形状样本投影到低维空间保持这种线性重构关系如图左所示方法希望保持数据在低维空间的映射之后能够保持流形上的测地线距离即全局的几何结构如图右所示图流形学习左保持局部的几何信息右保持全局的距离信息图片分别来自于论文和再如在计算机图形学和几何处理领域我们做的很多映射比如三维网格曲面的参数化也需要保持曲面上的某些几何性质保角度保面积保形等也发展出许多不同的求解映射的方法和技巧例如以下两个工作是我们做的有关三维曲面参数化到二维平面的工作第一个工作是曲面参数化工作的经典方法之一已经被集成到公司的建模软件以及著名的计算几何算法库中第二个工作是今年我们最新的一个工作能快速生成无翻转和低扭曲的网格参数化论文即将在下周的会议上汇报在计算机图形学中有时我们也需要将三维空间的二维流形曲面映射到另一个规则二维流形曲面比如球面上这时称为球面参数化如读者有兴趣可阅读我们如下的篇有关球面参数化的工作另外在计算机图形学及计算机辅助几何设计领域我们还有独特的曲线和函数的构造方法如通过基函数构造的曲线曲面通过样条基构造的样条曲线曲面分段曲线曲面以及更一般的非均有有理样条曲线曲面通过控制顶点就能很直观控制曲线曲面的形状而且发展出非常完备的理论及设计方法如图所示这些独特的曲线曲面的构造方法也是我们领域独特的已成为现代工业中产品曲面造型的工业标准标准数学表达图曲线曲面产品曲面造型的工业标准图片来源与互联网九实战深度神经网络学习由于本文是面向非机器学习专业介绍深度学习的因此笔者在这里只是大致介绍一下各种基于深度神经网络的学习方法的实际技巧不详细展开讨论有兴趣的读者可查阅更专业的书籍或论文看懂各种深度神经网络结构从数学上来看深度神经网络仅仅是一种函数的表达形式是复杂的多层复合函数由于它有大量的可调参数而且近年来随着大数据优化算法和并行计算硬件的发展使得用大规模的神经网络来逼近和拟合大数据成为可能至今不同领域的研究工作者及产业界工作者已将深度神经网络应用到各行各业的各个问题而且提出了各种形式各种结构的深度网络结构使人眼花缭乱似乎难以捉摸但事实并非如此万变不离其宗神经网络就是一个多层复合函数网络结构发生改变就是说函数的复合形式发生了变化因此只要掌握了这条原则各种复杂的神经网络你就能轻松看懂亦可参考互联网上的一文看懂个神经网络模型一文图多层神经网络所表达的函数可以看成前面的若干层网络所表达的函数蓝色与后面若干层网络所表达的函数红色的复合简单地说看懂各种神经网络结构只需看懂如下个要素具体解释就不详细展开了神经元与激活函数神经元接收什么数据通过什么样的激活函数输出数据人们发明了许多不同种类的神经元比如池化神经元和插值神经元概率神经元循环神经元长短期记忆神经元门控循环神经元等不管什么样形式和名称的神经元我们只要搞明白神经元是通过什么样的函数来将接收数据变为输出数据不同之处就在接收数据和激活函数不同而已如图右所示神经网络结构不同的神经网络就是不同的多层复合形式的函数比如卷积神经网络就是一种限于局部感受野来改造的神经网络线性组合特殊化为局部区域的卷积操作且增加了池化层特别适合于文本图像及视频类数据的深度学习再比如残差神经网络有一种特殊的连接可以把数据从某一隐层传至后面几层通常是到层该网络的目的不是要找输入数据与输出数据之间的映射而是致力于构建输入数据与输出数据输入数据之间的映射函数本质上是在学习映射函数的一阶差分一阶导数能够有效抑制优化过程中的梯度消失类似地实质上是在学习映射函数的高阶差分因此也能有效抑制梯度消失现象同样地其他的各种神经网络结构也很容易看懂的比如等神经网络的组合一个多层的神经网络就是一个多次复合的函数中间的任何一层的输出都可以看成为原始输入数据的一个特征如图所示一个多层神经网络所表达的函数可以看成前面的若干层网络所表达的函数蓝色与后面若干层网络所表达的函数红色的复合因此在任何的一个中间层可以修改输出的特征向量可以插入一个其他的神经网络将该特征向量作为输入或与其他网络的特征向量进行叠加运算等等等如果将神经网络看成为积木整个组合过程就像在搭积木这样就可以发明和产生无穷无尽的网络结构比如并行网络累加网络等等很多网络名称在实际应用中需要根据问题本身的特点来合理设计网络的结构具体哪种网络结构合理有无最好都是组要根据问题本身思考和试验的主要是要看损失函数的误差以及在应用中的效果来决定的这个我们在后面还会进一步讨论如何调试深度神经网络一个深度神经网络所表达的函数具有成千上万个参数为了让损失函数在训练数据上达到最小就需要设计合理的网络来拟合这些数据在优化的过程中要保证迭代收敛也是必要的一般地调试较大的深度神经网络所花费的时间和劳力比调试传统程序要多而且要有耐心因为如果最后的结果不好损失函数很大或者不收敛如果确认你的代码无那么原因只有一个就是你所调的网络还不够好在调试神经网络的过程中有太多不确定的因素需要你去仔细思考并小心翼翼地去调试而这些大部分都需要靠直觉经验和少许运气来调试决定的比如需要选取多大的神经网络也就是选用什么样的拟合函数具体地网络要多少层每层多少节点这个只能凭直觉或经验了简单点的话就套用现成的网络结构或者再经过一些修改即可从函数的自由度来看网络的参数个数与数据大小不应相差太大初始化对于高度非线性的函数的最优化一个非常重要的因素就是如何挑选一个好的初始化在优化过程中如何判断优化过程是否收敛是否发生了过拟合以下有些简单的小经验分享下一定要可视化你的结果这样有助于你在训练过程中发现问题你应该明确的看到这些数据损失函数的变化曲线权重直方图变量的梯度等不能只看数值一定要可视化网络不是越深越好不要一上来就用等标准的大网络可能你的问题只要几层就能解决问题先建立一个较小的网络来解决核心问题然后一步一步扩展到全局问题先用小样本做训练有了感觉后再调试大的训练数据如果在几百次迭代后迭代还没开始收敛那么就要终止迭代不要傻等了要考虑修改你的网络了注意避免梯度消失或梯度爆炸另外在互联网上还能找到许多网友们分享的实用调网络的经验读者可自行去查找学习值得提醒的是上述分享的各个经验没有绝对的对与错须将它们当作经验去学习和使用即可需要根据你自己的实际问题来设计和调试你的深度神经网络即便对于行家来说调试较大的神经网络也是一项艰巨的任务数百万个参数在一起决定的函数一个微小的变化就能毁掉所有辛勤工作的成果然而不进行调试以及可视化一切就只能靠运气最后可能浪费掉大把的青春岁月正因为这些原因调试深度神经网络被认为是个不可解释的黑箱子优化网络的过程成为调参也被戏称为炼丹是个需要丰富经验的技术活深度学习开发工具要学习和使用基于深度神经网络的深度学习门槛并不高你大不必从头开始自己搭建神经网络系统和实现优化算法近几年来很多大公司和科研机构都在研究自己的深度学习框架且推出了不少深度学习开发平台如图深度学习的开发框架和工具也越来越多使得应用深度学习的入门门槛越来越低使用好的开发工具可让使用者减少底层开发的工作量而将重点关注于深度学习应用逻辑的开发及模型的优化上提高开发效率图基于深度神经网络的深度学习的开发工具图片来自互联网这里简单介绍几款大家常用的深度学习开发工具包基本都是开源的由公司发布支持多及多并行化运行并支持等主要的深度学习模型社区人气最火的深度学习开源项目由加拿大实验室发布使用最广泛的深度学习工具之一提供等语言接口基于脚本语言的工具支持等嵌入式平台基于语言开发的深度学习开源仿真平台基于语言开发的底层库使用或其模块化特性非常适合刚入门的初学者快速实验并测试深度学习网络的性能同时也开放提供对底层的修改关于这些开发工具包的使用互联网上包括有非常丰富的教材和使用文档读者有兴趣可以自行查阅学习深度神经网络专用芯片深度神经网络专用芯片使用的专用芯片是一种特殊用途的处理器它能够使得深度神经网络的训练更快具有更高的性能和更低的功耗专用芯片在性能和能源消耗方面的好处是显著的专用芯片的广泛使用还需要神经网络体系结构的标准化和对不同框架的支持国外的一些公司包括等公司已经开发和部署了专用芯片为基于的应用程序提供了极高的性能国内也有不少公司如寒武纪阿里巴巴等在从事专用芯片的研发也已经达到国际前沿水平十深度神经网络的能力与局限深度学习真有智能吗遗憾的是没有基于深度神经网络的深度学习的本质再回顾在本文笔者从函数逼近论的角度已详细剖析了基于深度神经网络的深度学习归纳为以下个基本点深度神经网络是一个复合函数对于任何单变量函数只要神经元足够多神经网络函数就能任意逼近它对于任何多变量函数一定可以被多个单变量函数的复合来逼近因此就能被深度神经网络函数进行逼近上述两点为神经网络函数的表达能力提供了理论基础基于的深度学习是个数据拟合最小二乘回归的过程拟合过程是在学习基函数多层映射是在做多次变换或参数化因此基于的深度学习是可以很清楚被理解的具体的应用过程中选多少层选多少基的问题在逼近论中就是没有很好的解决方案这是逼近论中就存在的问题并不是深度网络带来的问题从数学本质来看深度学习是在做最小二乘回归其他的机器学习方法也是基于最小二乘回归的其他回归方法或者统计方法从应用性来看还有以下一些对基于深度神经网络的深度学习的本质的说明基于的深度学习实际上在做拟合和统计网络结构设计需要靠直觉技巧和经验以及运气激活函数和损失函数的选择也很重要的层数各层的神经元个数决定了该函数的参数一般的都具有非常多的参数适合拟合大规模数据对小规模数据不建议用任何一个中间隐层的输出向量都可以作为输入数据的特征在该维数上的也是输入数据的参数化可以任意拼装组合成各种复杂的网络结构大部分论文中的结果都可能是过拟合因此很容易出现误判的情况输入的轻微变化比如给图片加少量噪声结果就相差很大容易犯大错是以成败论英雄的需要在实际应用中好才是真的好而这个永远不知道需要再提一下的是本文所讨论的基于的深度学习只是深度学习的一种方法而已还有其他的深度学习比如南京大学周志华老师团队所提的深度森林等方法深度神经网络具有智能吗从上面的剖析来看基于的深度学习实质上就是对样本数据集的拟合而已显而易见它仅仅是一个由数据驱动的产生的拟合器或分类器根本不是一个认知系统我们并没有看出来它有任何智能即对数据所呈现的规律知识的理解的存在对人工神经网络的研究可能仍然是几十年前的进展只不过现在网络的规模变大了数据大了优化强了而已从逼近论的角度来看如果数据足够大多到足以稠密覆盖整个目标函数那么简单的最近邻查找或者局部线性插值的性能就足够好了此时笔者猜测深度学习的性能与最近邻查找的性能可能相差无几举个例子市面上出现了许多种类的所谓的智能机器人声称可以与人进行自由对话建议读者去测试一下这些对话机器人你们就会发现这些机器人的问答系统基本没有理解只有少数有少量简单的理解如果对于在其数据库中存在的问题或类似的问题机器人能够回答得比较合理如果遇上需要逻辑推理的问题这些机器人基本就答非所问或者王顾左右而言他甚至开始卖萌了有空去逗逗这些对话机器人也是蛮有意思的事对于自动翻译等机器人只要有足够的两种语言的对应词条或句子比如有些公司的词库数据库非常庞大达到亿级就能翻译拟合得还不错达到一定的实用水准但从本质上来讲这些翻译算法并没有真正理解语言本身关于深度学习我们要有清醒的认识深度学习模型需要大量的训练数据才能有较好的效果因为这些学习模型都在做数据拟合或回归或者是在做统计实际问题中往往会遇到小样本问题采用传统的简单的机器学习方法则可以很好地解决了没必要非得用复杂的深度学习方法神经网络看起来来源于人脑神经元的启发但绝不是人脑的模拟它只是一个复合函数举个例子给一个三四岁的小孩看过几只猫之后他就能识别其他的猫也就是说人类的学习过程往往不需要大规模的训练数据而现在的深度神经网络的学习方法显然不是对人脑的模拟这几年各种媒体的大量夸张的报道和宣传即使是战胜人类的围棋算法也并不是真正的人工智能而仅仅是机器在一定的游戏规则下的策略计算能力极度夸大了人工智能的进展让很多大众误认为强人工智能已经来临甚至产生不必要的恐慌事实上现在的人工智能的进展仍然很小正如清华大学人工智能研究院院长张钹院士在年全球人工智能与机器人峰会上的报告上所指出的人工智能刚刚起步离真正的人工智能还很遥远我们必须走向具有理解的人工智能这才是真正的人工智能深度学习在工业界的应用虽然近几年发展起来的基于的深度学习并未产生真正的智能但由于现在有大量的数据基于数据驱动的深度学习方法可以在很多领域确实得到了落地的应用特别是在计算机视觉领域比如人脸识别图像分割与识别等该领域做了很多年的一些问题的研究工作瞬间就被深度学习所击败深度学习取得了巨大的成功因此我们仍能使用深度学习解决不少问题近几年催生了大量的人工智能公司分布在安防医疗金融智能机器人自动驾驶智能芯片智能家居等领域各高校也纷纷构建了人工智能研究院或大数据学院等各学会成立了人工智能相关的专业委员会国家也纷纷出台了有关人工智能的政策与文件包括新一代人工智能发展规划鼓励人工智能方面的人才培养从娃娃抓起和产业落地在科研领域大量的研究工作者和学生都涌进到人工智能领域在一些人工智能或计算机视觉领域的会议的投稿量迅速增加参加会议的人数达到历史新高但残酷的事实是绝大部分的人工智能公司都是伪人工智能他们或者仅仅在做着数据标注的工作因为标注数据的多少决定着拟合函数的智能另外已经有些公司因未能找到合适的盈利模式而倒闭前几年对深度学习及智能的期望并未达到人们及投资者的预期目标今年年初李开复曾指出年是泡沫破裂之年从两年前人工智能热潮的爆发到现在热潮逐渐退去人们也越来越趋于冷静来认识人工智能对其研究与应用逐渐趋于务实笔者也希望人工智能能真正快速发展让人类的生活变得越来越美好深度学习与人工智能的关系年几个计算机科学家相聚在达特茅斯会议提出了人工智能的概念梦想着用计算机来构造复杂的拥有与人类智慧同样本质特性的机器人工智能研究领域范围很广包括专家系统机器学习进化计算模糊逻辑计算机视觉自然语言处理推荐系统等如图左机器学习是一种实现人工智能的方法其基本做法是使用算法来解析数据从中学习然后对真实世界中的事件做出决策和预测与传统的为解决特定任务硬编码的软件程序不同机器学习是用大量的数据来训练通过各种算法从数据中学习如何完成任务传统的机器学习算法包括决策树聚类贝叶斯分类支持向量机等等从学习方法上来分机器学习算法可以分为监督学习如分类问题无监督学习如聚类问题半监督学习集成学习深度学习和强化学习等深度学习是一种实现机器学习的技术是解决特征表达的一种学习过程现在常指利用深度神经网络的深度学习在很早就有人提过但由于当时训练数据量不足计算能力落后因此深度学习的效果不尽如人意直到近几年深度学习才较好地实现了各种任务事实上除了基于深度神经网络的深度学习还有其他形式比如深度森林的深度学习方法因此机器学习是一种实现人工智能的方法深度学习是一种实现机器学习的技术而深度神经网络是实现深度学习的一种具体的实现工具如图右所示简而言之从范畴来讲他们的关系如下深度神经网络深度学习机器学习人工智能图左人工智能的研究范畴右人工智能机器学习和深度学习的关系图片来自互联网但残酷的事实是近几年流行的基于深度神经网络的深度学习仅仅是一个数据拟合器并不存在着分析与理解等智能的能力现有的技术和方法离真正的人工智能还非常遥远十一三维几何数据在计算机图形学中我们研究的对象是三维形状对象即虚拟空间中的数字化的物理实体表达对象的几何数据是继声音图像视频之后的第四代数字可视媒体如图已逐渐具有越来越广泛的应用图四代可视媒体的发展声音图像视频几何数据的重要性地球上的人及所有动物都有两只眼睛每只眼睛类似于一台照相机看到的是场景在视网膜成像平面上的投影图像如图左两只眼睛位置不同就产生对同一场景的视差从而在大脑生成了场景的立体和信息双目立体视觉原理如图右图左单眼看到的是图像为三维世界在成像平面上的投影影像右人类及动物的两只眼睛通过双眼的视差产生了对场景物体的立体和三维的信息图片来自于互联网因此人类及动物的视觉感知系统是具有立体的特性世界是三维的模型提供了世界及所有物体的全方位全息的表达能提供比二维图像更丰富更全面的空间信息比如猎鹰能在几公里外判断草地上一只兔子的远近和方位以便做出下一步的飞行行为而单眼如果以前存在的话动物则因无法分辨物体的远近而无情地被自然法则所淘汰计算机视觉通过对图像的理解和分析已经能够解决非常多的任务比如物体检测分割识别检索等但在很多实际应用中如果涉及到空间关系分析以及与环境和物体的理解交互和创造则光靠图像信息是不够的此时必须依赖场景的信息比如数字城市中的空间关系分析机器人抓取物体虚拟和增强现实物联网数据的空间关联等如图所示在这些应用中仅仅靠图像信息是无法很快完成各项任务的图三维数据在很多实际应用场景中是必不可少的信息数据集随着物体的扫描与采集手段的日益增多特别是近几年快速发展的深度相机比如等未来可采集信息的移动终端去年上已有深度相机的普及以及互联网上模型的创造与分享数据包括数据将日益丰富至今已有许多研究机构构建并公布了一些数据集比如数据集包含了约多万个模型多类物体每个模型有语义注释这是当前最大的一个三维模型数据集其子集具有一些手工的标注数据集包含了多个模型类包含两个子集和数据集包含了近个非刚性模型用于检索测试数据集包含了上千个非刚性模型包括不少人的模型和合成数据集数据集由大学和大学公布主要是人体模型数据还有一些高校和研究机构公开了一些数据集包括大学个室内场景万个标记图像纽约大学个室内场景大学个办公室场景个居家室内场景大学个室内场景等另外瑞士苏黎世大学发布了一个使用激光扫描扫描的个办公室场景的高精度场景模型数据集形状描述子类似于图像人们也需要对模型数据进行分类与识别分割与建模匹配与检索等完成这些任务的关键就是如何定义和计算模型的形状描述子或特征至今人们已提出了各种基于全局和局部的形状几何属性的空间分布或统计信息的形状描述子如图所示这些形状描述子都是针对某种特定的应用或某些特定的模型类型而提出的同时满足所有应用及所有模型类型的形状描述子还没有在实际应用中应根据物体类型和具体应用来选择合适的形状描述子特征选择比如笔者如下论文使用稀疏选择的方法来选择合适的形状描述子用于形状分割几何数据的深度学习的挑战随着数据包括单个模型以及场景模型的日益流行和丰富如何利用上述丰富的数据集来完成分析和处理形状数据已成为研究工作者及产业工作者的关注焦点和研究热点因此形状的特征提取方法也逐步从上述的人工定义特征方法到基于深度学习的特征学习方法的发展相对于图像与视频处理几何数据的深度学习方法在近几年才开始有了一些工作和进展但仍面临着一些挑战数据集较小相比于等千万级数据量的图像数据集现在的模型数据集的数量很少仍未达到大数据的规模另外数据的标注也是较为困难的数据是非结构化的图像和视频都是结构化数据是线性结构的可以表达为一个向量或矩阵但模型数据是非结构化的如图所示每个顶点的邻域数是非固定的从拓扑上看是一个图无法直接使用深度神经网络数据的复杂性相对于图像和视频模型还有其他一些复杂性包括正向姿态性图像基本都是正向拍摄的但是模型的姿态可以是任意的如何消除姿态的影响仍有挑战为了方便一般会预先将数据集中的模型统一正朝向表达不统一图像就是一个矩阵结构表达非常统一但是模型有不同的表达方式比如点云网格体素隐函数树等如图使得需要对不同表达的数据进行不同的处理数据不完整由于采集设备的精度问题有些数据比如数据中存在着大量的噪声和游离点由于遮挡关系有些数据不够完全存在着空洞还有些数据的采样密度不均匀造成不同部位的信息不对称图各种人工形状描述子图二维图像与三维网格的数据结构的区别图三维几何数据的不同表达十二智能图形处理三维数据的深度学习图形作为一种新型的可视媒体数据类型同样可以利用人工智能和深度学习的方法来进行分析和处理融合大数据并行计算及人工智能技术的最新进展以提高计算机图形学算法及系统易用性及效率称为智能图形处理近几年已有越来越多的研究工作将机器学习和深度学习的方法应用于数据本节仅就形状描述子方面做一个简略的介绍应用深度学习来自动抽取形状特征主要有以下种方法第一种基于传统的人工特征维数一致了来进行更抽象的形状特征的学习和抽取这种方法是非端到端的第二种将数据转化为规整结构数据欧氏区域然后再应用深度学习方法抽取形状特征称为显式方法第三种将深度神经网络改造成能够处理非欧氏区域数据称为隐式方法下面分别从方法论上进行简单介绍基于人工特征的特征学习方法由于传统卷积无法直接作用于形状早期的方法避开了直接以形状作为输入的训练而是先从模型中提取一些人工特征称为低级特征如图再把这些特征输入到神经网络中进行学习获得新的抽象特征称为高级特征如图所示图基于人工特征的特征学习方法人工定义的特征很大程度上取决于人的经验因此基于人工特征的特征学习方法并不是端到端的深度学习方法另外各种人工特征向量的排列也会改变特征矩阵及其卷积后的结果对结果产生如何的影响也不清楚非本征方法这种方法是将数据进行合适的变换将其变换到一个欧氏空间的规则区域以便适用于深度神经网络此类方法改变了网格模型的形态本质上也改变了数据分布空间是为了适应传统或对拓扑的要求而提出的折衷解决方法因此成为基于欧氏空间的学习方法或非本征方法如图所示图非本征深度学习方法变换的区域和方法主要有以下几种图像区域最简单的方法就是将模型数据投影到平面形成图像多视图图像将模型往多个视角方向投影生成多幅图像比如全景图像将模型往平面投影生成一幅全景图比如参数化图像将模型的参数化看成为一幅图像比如体素区域类比于图像中的像素将形状看作三维体素网格的分布比如由于体素的个数是三次方增长因此体素的分辨率无法太大一般连的分辨率做起来都很困难只将模型表面用体素来表达而省去内部体素的存储并且利用八叉树的形式来存储可以有效地提高体素的分辨率及模型的表达精度比如基元组合三维形状还可以表示成一些基本单元的组合如矩形块圆柱圆锥球等然后利用网络来学习这些基本体块的参数比如点云直接将数据看成点的集合将每个点看作一个神经元节点节点包含点的坐标或法向等信息然后利用深度神经网络提取点的特征这里要克服点及点邻域的顺序无关性等比如本征方法本征方法直接将三维形状看成二维流形或由点组成的图顶点之间的距离不再是欧氏距离然后直接将卷积定义在这样的数据结构上称为非欧氏空间学习的方法或本征方法如图两种典型的网络为测地卷积网络和图卷积网络如图可参考图本征深度学习方法图本征深度学习的具体方法左测地卷积右图卷积网络端到端的生成模型在上面的工作中深度学习用于解决形状的识别分割匹配和对应等问题近年来越来越多的工作致力于模型的构建和生成大部分都是使用生成对抗网络由于可以自动生成大量的模型对于扩充模型数据集具有重要的意义非常值得大家关注这里简略介绍一下端到端的模型生成的一些工作所谓端到端的生成模型就是输入是简单易获得的输出是三维模型中间无需其他操作例如可以从输入的一张照片输出三维体素模型点云模型或者网格模型及其参数化利用级联的图卷积网络由粗到精逐渐细化网格模型由手绘草图生成参数化的夸张人脸模型后记此文分享了笔者对基于深度神经网络的深度学习的理解从逼近论的角度深入浅出地理解了深度学习的原理了解了整个调试深度网络的炼丹过程是个需要丰富经验的技术活机器学习还有很多其他重要的方法比如非监督学习和强化学习等笔者就不一一进行介绍了从本文的分析可知基于深度神经网络的深度学习背后的运行原理主要基于数据驱动的函数拟合采用的是经验主义的实用方法比如数学中的最小二乘方法及统计学中的统计回归方法等它离真正的人工智能即能通过图灵测试的智能机器还很远从数学本质上来看现在基于数据的深度学习方法实质上只有记忆能力并没有对问题的理解能力更没有人们所期待的人类智能从研究的方法论来看对于要研究的对象信息或数据如果在不同的空间角度来观察会有对该对象有不同的理解和认知比如信号处理中在时域和频域的变换下的分析即对象在该空间中所表现的特征因此在不同的空间进行变换就成为关键的步骤之一在以往的研究中需要研究者对对象有深刻的理解来对信息进行特定的变换即在特定的基函数的变换下而深度神经网络实现了端到端的变换通过多层深度复合的变换多次变换自动提取了在不同空间的特征即根据数据本身自适应学习基函数这是深度神经网络的巨大优势之一另外深度神经网络的另一个巨大优势就是可以通过数量众多的小函数激活函数的线性变换及复合来逼近非常复杂的函数解决了人工设计基函数的困难因此基于深度神经网络的深度学习具有非常广泛的实际应用另外值得一提的是从变换到小波变换的发展来看可以将神经网络中的激活函数变为特殊的局部基函数使得比现有的全局激活函数适用于更广泛的函数比如瞬时信号函数将来再抽时间来详细分析和阐述从计算数学领域来看神经网络提供了可以构建任意维度之间的映射和函数这是传统逼近论所无法做到的这使得人们研究的数据维度和数据类型极大的丰富了而不只是对维和维等低维数据进行分析和处理这将是一个非常重要的方法论将对科学领域的研究方法手段和进展也具有极大的促进和推动作用有理由相信深度学习方法或人工智能方法将会被当作计算数学应用科学的计算工具或科研工具催生新的科研范式被越来越多地用来解决一些计算数学和其他科学领域中的问题从方法论来看也可以从统计学的角度来看深度神经网络将深度神经网络来拟合数据的分布分布函数比如生成网络和等也是近期研究的热点在本文中对此方法论没有进行展开图人工智能技术成熟度曲线图片来源于互联网五年前年月麻省理工学院技术评论杂志将深度学习列为年十大突破性技术之首经过前几年的高速发展根据在月刚发布的人工智能技术成熟度曲线如图深度神经网络及深度学习已从前几年的爬坡阶段到达顶部未来相信人们将会更冷静地来看待及发展相关的技术和应用最近有人提出可微编程的概念就是将神经网络当成一种语言而不是一个简单的机器学习的方法来描述我们客观世界的规律甚至曾在的文章中说道深度学习已死可微编程永生这个概念的提出又将神经网络提高了一个层次本文完稿核对时笔者发现文中有些数学符号比如等存在着重复定义与使用但根据上下文并不影响阅读和理解因此笔者就不花时间去修正了在此说明下再次声明下笔者的主要研究领域为计算机图形学而非人工智能领域因此本文仅仅为笔者从外行的角度对基于深度神经网络的深度学习的粗浅理解而非人工智能领域对深度神经网络和深度学习的权威解释笔者对其中的有些内容的理解是有限的甚至是有误的因此该文仅供读者参考不作为专业资料如有不当之处还请读者指正致谢笔者在学习深度神经网络及深度学习的过程中阅读了许多相关的书籍和论文比如周志华老师的机器学习一书和李宏毅老师的课件等以及微信订阅号比如人工智能学家深度学习大讲堂老顾谈几何等在此表示感谢也感谢舒振宇徐凯韩晓光沈小勇胡瑞珍等同行以及笔者实验室的同事张举勇傅孝明杨周旺等和学生陈明佳方清杜冬石磊欧阳文清刘中远等与他们的交流和讨论也让笔者受益匪浅最后希望您能从本文受益祝您健康快乐成功刘利刚中国科学技术大学图形与几何计算实验室个人主页电子邮箱年月日',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-12-10 16:01:57',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Find everything you need" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">Find everything you need</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/GDAL/" style="font-size: 1.05rem;">GDAL<sup>4</sup></a><a href="/tags/GEE/" style="font-size: 1.05rem;">GEE<sup>1</sup></a><a href="/tags/GIS/" style="font-size: 1.05rem;">GIS<sup>8</sup></a><a href="/tags/IDL/" style="font-size: 1.05rem;">IDL<sup>1</sup></a><a href="/tags/JavaScript/" style="font-size: 1.05rem;">JavaScript<sup>1</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>11</sup></a><a href="/tags/SAR/" style="font-size: 1.05rem;">SAR<sup>2</sup></a><a href="/tags/Sentinel5/" style="font-size: 1.05rem;">Sentinel5<sup>3</sup></a><a href="/tags/arcgis/" style="font-size: 1.05rem;">arcgis<sup>1</sup></a><a href="/tags/cpp/" style="font-size: 1.05rem;">cpp<sup>2</sup></a><a href="/tags/deep-learn/" style="font-size: 1.05rem;">deep_learn<sup>1</sup></a><a href="/tags/ffmpeg/" style="font-size: 1.05rem;">ffmpeg<sup>1</sup></a><a href="/tags/gdal/" style="font-size: 1.05rem;">gdal<sup>5</sup></a><a href="/tags/gis/" style="font-size: 1.05rem;">gis<sup>5</sup></a><a href="/tags/golang/" style="font-size: 1.05rem;">golang<sup>1</sup></a><a href="/tags/himawari8/" style="font-size: 1.05rem;">himawari8<sup>1</sup></a><a href="/tags/kotlin/" style="font-size: 1.05rem;">kotlin<sup>3</sup></a><a href="/tags/leetcode/" style="font-size: 1.05rem;">leetcode<sup>1</sup></a><a href="/tags/linux/" style="font-size: 1.05rem;">linux<sup>6</sup></a><a href="/tags/matlab/" style="font-size: 1.05rem;">matlab<sup>4</sup></a><a href="/tags/pyside6/" style="font-size: 1.05rem;">pyside6<sup>2</sup></a><a href="/tags/python/" style="font-size: 1.05rem;">python<sup>47</sup></a><a href="/tags/rust/" style="font-size: 1.05rem;">rust<sup>1</sup></a><a href="/tags/sar/" style="font-size: 1.05rem;">sar<sup>5</sup></a><a href="/tags/shp/" style="font-size: 1.05rem;">shp<sup>1</sup></a><a href="/tags/vs/" style="font-size: 1.05rem;">vs<sup>1</sup></a><a href="/tags/%E4%B8%8B%E8%BD%BD/" style="font-size: 1.05rem;">下载<sup>1</sup></a><a href="/tags/%E5%8C%80%E8%89%B2/" style="font-size: 1.05rem;">匀色<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">工具<sup>2</sup></a><a href="/tags/%E6%8A%80%E6%9C%AF/" style="font-size: 1.05rem;">技术<sup>3</sup></a><a href="/tags/%E6%A1%88%E4%BB%B6/" style="font-size: 1.05rem;">案件<sup>1</sup></a><a href="/tags/%E6%B3%A8%E5%86%8C%E6%B5%8B%E7%BB%98%E5%B8%88/" style="font-size: 1.05rem;">注册测绘师<sup>13</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">深度学习<sup>15</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 1.05rem;">爬虫<sup>1</sup></a><a href="/tags/%E7%9F%A2%E9%87%8F/" style="font-size: 1.05rem;">矢量<sup>26</sup></a><a href="/tags/%E7%BB%98%E5%9B%BE/" style="font-size: 1.05rem;">绘图<sup>1</sup></a><a href="/tags/%E7%BC%96%E7%A8%8B/" style="font-size: 1.05rem;">编程<sup>19</sup></a><a href="/tags/%E9%81%A5%E6%84%9F/" style="font-size: 1.05rem;">遥感<sup>184</sup></a><a href="/tags/%E9%81%A5%E6%84%9F-%E7%9F%A2%E9%87%8F/" style="font-size: 1.05rem;">遥感,矢量<sup>1</sup></a><a href="/tags/%E9%97%B2%E8%81%8A/" style="font-size: 1.05rem;">闲聊<sup>2</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/07/"><span class="card-archive-list-date">七月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/06/"><span class="card-archive-list-date">六月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">10</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">19</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">14</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">13</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">无题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2024-12-10T08:01:31.304Z" title="发表于 2024-12-10 16:01:31">2024-12-10</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-12-10T08:01:57.532Z" title="更新于 2024-12-10 16:01:57">2024-12-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为广州"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>广州</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/"><header><h1 id="CrawlerTitle" itemprop="name headline">无题</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">ytkz</span><time itemprop="dateCreated datePublished" datetime="2024-12-10T08:01:31.304Z" title="发表于 2024-12-10 16:01:31">2024-12-10</time><time itemprop="dateCreated datePublished" datetime="2024-12-10T08:01:57.532Z" title="更新于 2024-12-10 16:01:57">2024-12-10</time></header><p><strong>什么是深度学习？</strong></p>
<p><strong>-</strong> <strong>45**</strong>分钟理解深度神经网络和深度学习**</p>
<p><a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu">刘利刚</a></p>
<p><a target="_blank" rel="noopener" href="http://gcl.ustc.edu.cn/">中国科学技术大学图形与几何计算实验室</a></p>
<p><a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu">http://staff.ustc.edu.cn/~lgliu</a></p>
<p><strong>【绪言】</strong></p>
<p>近年来，人工智能（Artificial Intelligence, AI）和深度学习（Deep Learning, DL）非常火爆，在各个领域得到了广泛的应用。在笔者所从事的<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/Resources/CG/What_is_CG.htm">计算机图形学</a>领域，也出现了越来越多的使用深度学习方法来解决各种问题的研究工作。2018年7月初，笔者首次在第七届<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/Courses/SummerSchool/USTC-summer-school.html">中国科学技术大学《计算机图形学前沿》暑期课程</a>上讲授和分享了笔者从数学（函数逼近论）的角度来对基于深度神经网络（Deep Neural Network, DNN）的深度学习的进行理解。之后，不断有学生来向笔者进一步询问和交流有关资料和问题。为了使学生们能够更好、更快理解和使用深度神经网络和深度学习，特撰写此文。</p>
<p>本文的目的是帮助非人工智能领域的学生（主要是计算机图形学领域的学生及笔者的学生）来搞懂深度学习（这里指狭义的深度学习，即基于DNN的深度学习）的基本概念和方法。笔者尝试用通俗的语言，从函数逼近论的角度来阐释深度神经网络的本质。由于笔者的主要研究领域为<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/Resources/CG/What_is_CG.htm">计算机图形学</a>，而非人工智能领域，因此本文仅仅为笔者从外行的角度对基于DNN的深度学习的粗浅理解，而非人工智能领域对DNN和深度学习的权威解释。因而，笔者对其中的有些内容的理解是有限的，甚至是有误的。如有不当之处，还请读者指正！</p>
<p><strong>一、</strong>  <strong>从数据拟合说起</strong></p>
<p>在大学的《数学分析》、《微积分》或者《数值分析》中，大家都已学习并熟悉数据拟合问题及求解拟合问题的最小二乘法。</p>
<p>1.1.      数据拟合问题</p>
<p>在科学技术的各领域中，我们所研究的事件一般都是有规律（因果关系）的，即自变量集合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image001.png" alt="img">与应变量集合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image002.png" alt="img">之间存在的对应关系通常用映射来描述<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image003.png" alt="img">（特殊情况：实数集合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image004.png" alt="img">到实数集合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image004.png" alt="img">之间的映射称为函数）。这样能根据映射（函数）规律作出预测并用于实际应用。</p>
<p>有些函数关系可由理论分析推导得出，不仅为进一步的分析研究工作提供理论基础，也可以方便的解决实际工程问题。比如，适合于宏观低速物体的牛顿第二运动定律<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image005.png" alt="img">就是在实际观察和归纳中得出的普适性力学定律。</p>
<p>但是，很多工程问题难以直接推导出变量之间的函数表达式；或者即使能得出表达式，公式也十分复杂，不利于进一步的分析与计算。这时可以通过诸如采样、实验等方法获得若干离散的数据（称为样本数据点），然后根据这些数据，希望能得到这些变量之间的函数关系，这个过程称为数据拟合（Data fitting），在数理统计中也称为回归分析（Regression analysis）。</p>
<p>这里值得提一下，在实际应用中，还有一类问题是输出的结果<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image002.png" alt="img">是离散型的（比如识别图片里是人、猫、狗等标签的一种），此时问题称为分类（Classification）。</p>
<p>1.2.      数据拟合类型</p>
<p>我们先考虑最简单的情形，即实数到实数的一元函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image006.png" alt="img">。假设通过实验获得了<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image007.png" alt="img">个样本点<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image008.png" alt="img">。我们希望求得反映这些样本点规律的一个函数关系<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image009.png" alt="img">，如图1所示。</p>
<p>如果要求函数严格通过每个样本点，即</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image010.png" alt="img"> ，</th>
<th>(1.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>则求解函数的问题称为插值问题（Interpolation）。</p>
<p>一般地，由于实验数据带有观测误差，因此在大部分情况下，我们只要求函数反映这些样本点的趋势，即函数靠近样本点且误差在某种度量意义下最小，称为逼近问题（Approximation）。若记在某点的误差为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image011.png" alt="img"></th>
<th>(1.2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>且记误差向量为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image012.png" alt="img">。逼近问题就是要求向量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image013.png" alt="img">的某种范数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image014.png" alt="img">最小。一般采用欧氏范数（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image015.png" alt="img">范数）作为误差度量的标准（比较容易计算），即求如下极小化问题：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image016.png" alt="img">.</th>
<th>(1.3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image017.jpg" alt="img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image018.jpg" alt="img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image019.jpg" alt="img"></p>
<p>图1. 数据拟合。左：输入的样本点；中：插值函数；右：逼近函数。</p>
<p>无论是插值问题还是逼近问题，一个首要的问题就是函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">的类型的选择和表示问题，这是《函数逼近论》中的一个比较“纠结”的问题。</p>
<p><strong>二、</strong>  <strong>函数逼近论简介</strong></p>
<p>函数的表示是函数逼近论中的基本问题。在数学的理论研究和实际应用中经常遇到下类问题：在选定的一类函数中寻找某个函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">，使它与已知函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">（或观测数据）在一定意义下为最佳近似表示，并求出用<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">近似表示<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">而产生的误差。这就是函数逼近问题。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">称为逼近函数或拟合函数。</p>
<p>在函数逼近问题中，逼近函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">的函数类可以有不同的选择；即使函数类选定了，在该类函数中确定<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">的方式仍然是各式各样的；<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">对<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">的近似程度（误差）也可以有各种不同的定义。我们分别对这些问题进行理解和讨论。</p>
<p>2.1.       逼近函数类</p>
<p>在实际问题中，首先要确定函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">的具体形式。这不单纯是数学问题，还与所研究问题的运动规律及观测数据有关，也与用户的经验有关。一般地，我们在某个较简单的函数类中去寻找我们所需要的函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">。这种函数类叫做逼近函数类。</p>
<p>逼近函数类可以有多种选择，一般可以在不同的函数空间（比如由一些基函数通过线性组合所张成的函数空间）中进行选择。如下是一些常用的函数类。</p>
<p>(1)  多项式函数类</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">次代数多项式，即由次数不大于<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">的幂基<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image023.png" alt="img">的线性组合的多项式函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image024.png" alt="img">,</th>
<th>(2.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image025.png" alt="img">为实系数。</p>
<p>更常用的是由<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">次Bernstein基函数来表达的多项式形式（称为Bernstein多项式或Bezier多项式）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image026.png" alt="img">,</th>
<th>(2.2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中Bernstein基函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image027.png" alt="img"></p>
<p>(2)  三角多项式类</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">阶三角多项式，即由阶数不大于<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">的三角函数基的线性组合的三角函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image028.png" alt="img">,</th>
<th>(2.3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image029.png" alt="img">为实系数。</p>
<p>这些是常用的逼近函数类。在逼近论中，还有许多其他形式的逼近函数类，比如由代数多项式的比构成的有理分式集（有理逼近）；按照一定条件定义的样条函数集（样条逼近）；径向基函数（RBF逼近）；由正交函数系的线性组合构成的（维数固定的）函数集等。</p>
<p>2.2.      万能逼近定理</p>
<p>在函数逼近论中，如果一组函数成为一组“基”函数，需要满足一些比较好的性质，比如光滑性、线性无关性、权性（所有基函数和为1）、局部支集、完备性、正性、凸性等。其中， “完备性”是指，该组函数的线性组合是否能够以任意的误差和精度来逼近给定的函数（即万能逼近性质）？</p>
<p>对于多项式函数类，我们有以下的“万能逼近定理”：</p>
<p><strong>【定理**</strong>2.1<strong> </strong>（<strong><strong>Weierstrass</strong></strong>逼近定理）】**对<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image030.png" alt="img">上的任意连续函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">，及任意给定的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image031.png" alt="img">，必存在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">次代数多项式<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image024.png" alt="img">，使得</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image032.png" alt="img"></th>
<th>(2.4)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Weierstrass逼近定理表明，只要次数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">足够高，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">次多项式就能以任何精度逼近给定的函数。具体的构造方法有Bernstein多项式或Chebyshev多项式等，这里不详细展开。</p>
<p>类似地，由Fourier分析理论（或Weierstrass第二逼近定理），只要阶数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">足够高，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">阶三角函数就能以任何精度逼近给定的周期函数。这些理论表明，多项式函数类和三角函数类在函数空间是“稠密”的，这就保障了用这些函数类来作为逼近函数是“合理”的。</p>
<p>2.3.      逼近函数类选择的“纠结”</p>
<p>在一个逼近问题中选择什么样的函数类作逼近函数类，这要取决于被逼近函数本身的特点，也和逼近问题的条件、要求等因素有关。</p>
<p>在实际应用中，这里其实存在着两个非常“纠结”的问题。</p>
<p>第一，选择什么样的逼近函数类？一般地，需要用户对被逼近对象或样本数据有一些“先验知识”来决定选择具体的逼近函数类。比如，如果被逼近的函数具有周期性，将三角函数作为逼近函数是个合理的选择；如果被逼近的函数具有奇点，将有理函数作为逼近函数更为合理，等等。</p>
<p>第二，即使确定了逼近函数类，选择多高的次数或阶数？比如，如果选择了多项式函数类，根据Lagrange插值定理，一定能找到一个<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image033.png" alt="img">次多项式来插值给定的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">个样本点。但如果<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">较大，则这样得到的高次多项式很容易造成“过拟合”（Overfitting）。而如果选择的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">过小，则得到的多项式容易造成“欠拟合”（Underfitting）。如图2所示。过拟合或欠拟合函数在实际应用中是没有用的，因为它们的预测能力非常差！</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image034.jpg" alt="img"></p>
<p>图2. 用不同次数的多项式拟合样本点（蓝色点）。</p>
<p>左：欠拟合；中：合适的拟合；右：过拟合。</p>
<p>这里有个概念需要提及一下。一个逼近函数“表达能力”体现在该函数的未知参数（即公式(2.1)-(2.3)中的系数）与样本点个数的差，也称为“自由度”。如果逼近函数的未知参数越多，则表达能力越强。然而，在实际的拟合问题中，逼近函数的拟合能力并非越强越好。因为如果只关注样本点处的拟合误差的话，非常强的表达能力会使得样本点之外的函数值远远偏离期望的目标，反而降低拟合函数的预测性能，产生过拟合，如图2 (右)所示。</p>
<p>人们发展出各种方法来减缓（不能完全避免）过拟合。比如，剔除样本点中的噪声（数据去噪）、增加样本点数据量（数据增广）、简化预测模型、获取额外数据进行交叉验证、或对目标函数进行适当的正则化等。在此不详细叙述。</p>
<p>在实际应用中，如何选择拟合函数的数学模型（合适的逼近函数类及其阶数），并不是一开始就能选好，往往须通过分析确定若干模型后，再经过实际计算、比较和调整才能选到较好的模型。需要不断的试验和调试（称为“调参”过程），是个需要丰富经验的“技术活”。</p>
<p>2.4.      最小二乘法（Least Squares Method）</p>
<p>假设通过分析我们已经确定了逼近函数类及其次数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">。记基函数（一般线性无关）为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image035.png" alt="img">。记<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image036.png" alt="img">为这些基函数所张成的线性空间（函数空间）。则逼近函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">可记为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image037.png" alt="img">,</th>
<th>(2.5)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image038.png" alt="img">为待定权系数。</p>
<p>关于最小二乘法的一般提法是：对给定的一组样本点数据<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image039.png" alt="img">，要求在函数类<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image040.png" alt="img">中找一个函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">，使误差的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image015.png" alt="img">模的平方<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image041.png" alt="img">，即式(1.3)，达到最小。</p>
<p>对于分析极小化误差(1.3)，可得关于系数向量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image042.png" alt="img">的法方程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image043.png" alt="img">,</th>
<th>(2.6)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>从而可求得</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image044.png" alt="img">.</th>
<th>(2.7)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>由于法方程是一个线性方程组，因此基于最小二乘法的函数求解也称为线性回归。</p>
<p>另外，我们可在误差项中加个权，表示不同点处的数据比重不同，此时称为加权最小二乘方法（Weighted least squares, WLS）。另外，还有移动最小二乘法（Moving least squares, MLS）等其他最小二乘法的改进方法。此处不详细叙述。</p>
<p><strong>三、</strong>  <strong>稀疏表达和稀疏学习</strong></p>
<p>在实际应用中， 2.3节中所述的两个“纠结”问题时有发生。人们发展出不同的方法来尝试解决。</p>
<p>3.1.      岭回归（Ridge Regression）</p>
<p>当数据量较少的情况下，最小二乘法（线性回归）容易出现过拟合的现象，法方程的系数矩阵<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image045.png" alt="img">会出现奇异（非满秩），此时回归系数会变得很大，无法求解。</p>
<p>这时在最小二乘法的结果（式(2.7)）中加一个小扰动<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image046.png" alt="img">，使原先无法求广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解，即</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image047.png" alt="img">.</th>
<th>(3.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>事实上，这个解对应于如下极小化问题的解：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image048.png" alt="img">.</th>
<th>(3.2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image049.png" alt="img">，参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image050.png" alt="img">称为正则化参数（岭参数）。上述回归模型称为岭回归，其与最小二乘法的区别在于多了关于参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image051.png" alt="img">的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image015.png" alt="img">范数正则项。这一项是对<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image051.png" alt="img">的各个元素的总体的平衡程度，即限制这些权稀疏的方差不能太大。</p>
<p>实际应用中，如果岭参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image050.png" alt="img">选取过大，会把所有系数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image051.png" alt="img">均最小化（趋向于0），造成欠拟合；如果岭参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image050.png" alt="img">选取过小，会导致对过拟合问题解决不当。因此岭参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image050.png" alt="img">的选取也是一个技术活，需要不断调参。对于某些情形，也可以通过分析选择一个最佳的岭参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image050.png" alt="img">来保证回归的效果，在此不详细叙述。</p>
<p>3.2.      Lasso回归（Least Absolute Shrinkage and Selection Operator）</p>
<p>Lasso回归的极小化问题为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image052.png" alt="img">,</th>
<th>(3.3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image053.png" alt="img">，正则项为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image054.png" alt="img">范数正则项。Lasso回归能够使得系数向量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image051.png" alt="img">的一些元素变为0（稀疏），因此得到的拟合函数为部分基函数的线性组合。</p>
<p>3.3.      稀疏表达与稀疏学习</p>
<p>根据Lasso回归的分析，我们可通过对回归变量施加<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image055.png" alt="img">范数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image056.png" alt="img">（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image055.png" alt="img">范数为元素中非0元素的个数，在很多时候可以用<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image054.png" alt="img">范数近似）的正则项，以达到对回归变量进行稀疏化，即大部分回归变量为0（少数回归变量非0）。这种优化称为稀疏优化。也就是说，对回归变量施加<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image054.png" alt="img">范数能够“自动”对基函数进行选择，值为0的系数所对应的基函数对最后的逼近无贡献。这些非0的基函数反映了的样本点集合的“特征”，因此也称为特征选择。</p>
<p>通过这种方法，为了保证防止丢失一些基函数（特征），我们往往可以多选取一些基函数（甚至可以是线性相关的），使得基函数的个数比输入向量的维数还要大，称为“超完备”基（Over-complete basis）或过冗余基，在稀疏学习中亦称为“字典”。然后通过对基函数的系数进行稀疏优化，选择出合适（非0系数）的基函数的组合来表达逼近函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">。</p>
<p>这在一定程度上克服了2.3节中所提出的两个“纠结”的问题。因为，这时可以选取较多的基函数及较高的次数，通过稀疏优化来选择（“学习”）合适的基元函数，也称为稀疏学习。另外，基函数（字典）和稀疏系数也可以同时通过优化学习得到，称为字典学习。</p>
<p>对于矩阵形式（比如多元函数或图像），矩阵的稀疏表现为矩阵的低秩（近似为核模很小），则对应着矩阵低秩求解问题。在此不详细叙述。</p>
<p>稀疏学习与最近十年来流行的压缩感知（Compressive sensing）理论与方法非常相关，也是机器学习领域的一种重要方法。其深厚的理论基础由华裔数学家陶哲轩（2006年国际数学家大会菲尔兹奖得主）、斯坦福大学统计学教授David Donoho（2018年国际数学家大会高斯奖得主）等人所建立，已成功用于信号处理、图像与视频处理、语音处理等领域。在此不详细展开介绍。</p>
<p>近年来，笔者及其团队成功地将稀疏学习的理论和方法推广用于三维几何建模与处理中，在计算机图形学领域已发表十余篇相关论文（包括在顶级期刊ACM Transactions on Graphics上发表了4篇论文）。有兴趣的读者可详细查看<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/">笔者的相关论文</a>：</p>
<p>[3-1] Xu et al. Survey on Sparsity in Geometry Modeling and Processing. Graphical Models, 82, 160-180, 2015. （三维几何处理的稀疏优化方法的综述论文）</p>
<p>[3-2] Wang et al. Construction of Manifolds via Compatible Sparse Representations. ACM Transactions on Graphics, 35(2), Article 14: 1-10, 2016. （基于基函数稀疏选择的曲面生成）</p>
<p>[3-3] Xiong et al. Robust Surface Reconstruction via Dictionary Learning. ACM Transactions on Graphics (Proc. Siggraph Asia), 33(6), Article 1: 1-12, 2014. （基于字典学习的曲面重建）</p>
<p>[3-4] Zhang et al. Local Barycentric Coordinates. ACM Transactions on Graphics (Proc. Siggraph Asia), 33(6), Article 1: 1-12, 2014. （基于稀疏优化的局部重心坐标）</p>
<p>[3-5] Hu et al. Co-Segmentation of 3D Shapes via Subspace Clustering. Computer Graphics Forum (Proc. Symposium on Geometry Processing), 31(5): 1703-1713, 2012. （基于特征稀疏选择的三维形状共同分割）</p>
<p>[3-6] Jin et al. Unsupervised Upright Orientation of Man-Made Models. Graphical Models (Proc. GMP), 74(4): 99-108, 2012. （基于矩阵低秩优化的三维形状正向计算）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image057.jpg" alt="img"></p>
<p>图3. 稀疏学习方法用于三维几何的建模与处理（笔者的工作）。</p>
<p><strong>四、</strong>  <strong>高维的逼近函数</strong></p>
<p>上面讨论了映射<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image003.png" alt="img">为一元函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image006.png" alt="img">的情形。上述的有关概念和方法同样可以推广至多元函数（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image058.png" alt="img">）及向量值函数（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image059.png" alt="img">）的情形（如图4）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image060.jpg" alt="img"></p>
<p>图4. 不同情形（维数）的逼近函数。</p>
<p>左上：一元函数；右上：多元函数；左下：一元向量值函数；右下：多元向量值函数。</p>
<p>4.1.      多元函数</p>
<p>假设有多元函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image061.png" alt="img">，即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image062.png" alt="img">，的一组测量样本数据<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image063.png" alt="img">。设<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的一组基函数为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image065.png" alt="img">。要求函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image066.png" alt="img">,</th>
<th>(4.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>使得误差</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image067.png" alt="img"></th>
<th>(4.2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>的平方和最小。这与式(1.3)和2.4节的极值问题完全一样。系数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image068.png" alt="img">, 同样通过求解法方程得到。上述过程称为多元函数的最小二乘拟合。</p>
<p>在函数论中，高维空间<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的基函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image065.png" alt="img">的构造也有许多方法。为了简便，在许多时候我们采用张量积形式的基函数，即每个基函数是各个维数的一元基函数的乘积形式。在计算机图形学和计算机辅助几何设计（CG&amp;CAGD）领域经常采用。</p>
<p>4.2.      向量值函数</p>
<p>如果因变量有多个，即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image059.png" alt="img">，称为向量值函数。比如空间螺旋线的参数方程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image069.png" alt="img"></th>
<th>(4.3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>就是一个从一维到三维的向量值函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image070.png" alt="img">。如果自变量的维数比应变量的维数低，则自变量可看作为应变量的参数。在式(4.3)中，变量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image071.png" alt="img">可看作为该螺旋线的参数。</p>
<p>对于向量值函数，可以看成是多个一元函数或多元函数（一般共享基函数）即可，所有处理方法（包括最小二乘法）都是对每个函数分别操作的，与前面单个函数的处理方法完全一样。如果用数学方式来表达，前面的很多符号由向量变为矩阵的形式而已。详细可查看《数学分析》。</p>
<p><strong>五、</strong>  <strong>参数曲线曲面拟合</strong></p>
<p>前面讨论的函数所能表达的曲线比较简单，无法表达封闭和多值的曲线。因此，在计算机图形学和计算机辅助几何设计中常用参数曲线曲面（本质是向量值函数，见4.2节）来表达几何形状，用于几何造型。另外，在计算机图形学中还有一种利用隐函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image072.png" alt="img">来表达曲线的方式，也常用于造型中，这里不展开介绍。</p>
<p>5.1.      二维参数曲线</p>
<p>从数学来看，一条曲线的本征维度是一维的，不论是从哪个维度的空间来看。从直观来看，该曲线能被“拉”成为一条直线或线段；即，一条有端点的曲线能和一条线段建立1-1对应，如图5（左）所示。因此，嵌入到二维空间中的曲线可由单个参数来表达，即有如下形式的参数形式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image073.png" alt="img"></th>
<th>(5.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image071.png" alt="img">为该曲线的参数，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image074.png" alt="img">, <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image075.png" alt="img">均为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image071.png" alt="img">的函数，分别表示曲线上对应于参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image071.png" alt="img">的点的两个坐标分量。一般参数曲线表达为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image076.png" alt="img">。本质上，这是一个从<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image004.png" alt="img">到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">的向量值函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image078.png" alt="img">。</p>
<p>5.2.      三维参数曲面</p>
<p>类似地，一张二维流形曲面的本征维度是二维的。一张曲面能与平面上的一块区域建立1-1对应，如图5（右）所示。因此，嵌入到三维空间中的曲面可由两个参数来表达，即有如下形式的参数形式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image079.png" alt="img"></th>
<th>(5.2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image080.png" alt="img">为该曲面的参数，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image081.png" alt="img">, <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image082.png" alt="img">均为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image080.png" alt="img">的二元函数，分别表示曲面上对应于参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image080.png" alt="img">的点的三个坐标分量。一般参数曲面表达为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image083.png" alt="img">。本质上，这是一个从<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">的向量值函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image085.png" alt="img">。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image086.jpg" alt="img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image087.jpg" alt="img"></p>
<p>图5. 参数曲线曲面。左：单参数曲线；右：双参数曲面。</p>
<p>5.3.      参数曲线曲面拟合</p>
<p>下面我们介绍如何使用参数曲线来拟合2D平面上的一组有序样本点。</p>
<p>参数曲线拟合的问题描述如下：给定平面上一组有序样本点<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image088.png" alt="img">，我们希望寻求一条拟合这些点的参数曲线关系<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image076.png" alt="img">。</p>
<p>如需决定一条拟合曲线，首先需要确定每个样本点<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image089.png" alt="img">所对应的参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image090.png" alt="img">。对于一组有序的样本点，所确定的一种参数分割，称之为这组样本点的参数化。参数化的本质就是找一组恰当的参数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image090.png" alt="img">来匹配这一组样本点，以便使这条拟合出来的曲线美观、合理。</p>
<p>在参数曲线拟合中，样本点的参数化是个非常基本而重要的问题。不同的参数化，对拟合曲线的形状会产生很大的影响。图6显示了三种参数化（均匀参数化、弦长参数化和中心参数化）对拟合曲线形状的影响。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image091.jpg" alt="img"></p>
<p>图6. 不同的参数化导致不同的参数曲线拟合结果。</p>
<p>类似地，如果利用参数曲面来拟合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">空间中给定的一组样本点，首先也需要确定这些样本点在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">平面上的参数化，如图7所示。在曲面参数化中，不仅需要建立样本点及其参数的一一对应，我们还希望参数区域的网格保持原始网格的一些重要的几何特性，比如角度、边长等。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image092.jpg" alt="img"></p>
<p>图7. 三维曲面的参数化。</p>
<p>曲面参数化是计算机图形学中非常基础且重要的一个研究课题，也一直是研究热点之一。笔者及所在的课题组也在这个课题方面作了大量的工作，有兴趣的读者可详细查看<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/">笔者的相关论文</a>：</p>
<p>（平面参数化）</p>
<p>[5-1] Ligang Liu, Lei Zhang, Yin Xu, Craig Gotsman, Steven J. Gortler. A Local/Global Approach to Mesh Parameterization. Computer Graphics Forum (Proc. Eurographics Symposium on Geometry Processing (SGP)), 27(5), 1495-1504, 2008.</p>
<p>[5-2] Ligang Liu, Chunyang Ye, Ruiqi Ni, Xiao-Ming Fu. Progressive Parameterizations. ACM Transactions on Graphics (Proc. Siggraph), 37(4), Article 41: 1-12, 2018.</p>
<p>（球面参数化）</p>
<p>[5-3] Xin Hu, Xiao-Ming Fu, Ligang Liu. Advanced Hierarchical Spherical Parameterizations. IEEE Transactions on Visualization and Computer Graphics, to appear, 2018.</p>
<p>[5-4] Chunxue Wang, Xin Hu, Xiaoming Fu, Ligang Liu. Bijective Spherical Parametrization with Low Distortion. Computers&amp;Graphics (Proc. SMI), 58: 161-171, 2016.</p>
<p>[5-5] Chunxue Wang, Zheng Liu, Ligang Liu. As-Rigid-As-Possible Spherical Parameterization. Graphical Models (Proc. GMP), 76(5): 457-467, 2014.</p>
<p>（流形参数化）</p>
<p>[5-6] Lei Zhang, Ligang Liu, Zhongping Ji, Guojin Wang. Manifold Parameterization. Lecture Notes in Computer Science (Proc. Computer Graphics International), 4035, 160-171, 2006.</p>
<p><strong>六、</strong>  <strong>从人工神经网络的观点来看拟合函数</strong></p>
<p>为了讲清楚人工神经网络（Artificial Neural Network，ANN），下面我们先从神经网络的观点来看传统的拟合函数和曲线曲面。</p>
<p>6.1.      一元函数</p>
<p>对于一元的逼近函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image006.png" alt="img"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image037.png" alt="img">.</th>
<th>(6.1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>可以看成为如图8的三层神经网络，其中只有一个隐层。隐层上有<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">个节点，激活函数分别为基函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image093.png" alt="img">（从这里我们将基函数称为“激活函数”）。输入层到隐层的权设为常值1（图8左），也可以看成为将输入层的值“直接代入”（不做任何变换，用虚线表示）到激活函数（图8右）。隐层到输出层的权为基函数的组合系数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image094.png" alt="img">。</p>
<p>中间隐层的输出节点所形成的向量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image095.png" alt="img">可看作为输入量的“特征”。</p>
<p>现在我们开始用机器学习的语言来描述数据拟合的过程。整个神经网络就是由中间节点的激活函数及权系数所决定的一个函数(6.1)。我们称样本点<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image008.png" alt="img">, 为训练数据（Training set），称函数在训练数据上的误差度量为损失函数（Loss function）。通过训练数据来极小化损失函数得到权系数的过程称为“训练”或“学习”。如果损失函数取为(1.3)的形式，则网络的训练过程本质上就是前面所讲的最小二乘法（2.4节）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image096.jpg" alt="img">   <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image097.jpg" alt="img"></p>
<p>图8. 一元逼近函数的神经网络。左：输入层到隐层的权系数均为常值1；</p>
<p>右：输入层到隐层看成为“直接代入”（用虚线表示）。</p>
<p>6.2.      多元函数和向量值函数</p>
<p>考虑一般的逼近函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image098.png" alt="img">。设<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的一组基函数为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image065.png" alt="img">。则函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">可看成为如下图的一个三层的神经网络。注意这里隐层的激活函数都是<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image007.png" alt="img">维函数，从输入层到隐层也是直接代入。输出层的各个分量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image099.png" alt="img">共享隐层的激活函数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image100.jpg" alt="img"></p>
<p>图9. 多元函数和向量值函数的神经网络。从输入层到隐层是变量直接代入到激活函数。</p>
<p>6.3.      拟合曲线曲面</p>
<p>拟合曲线可以看作为一个多层的神经网络，如下图所示。输入层（样本点）到参数化是一个变换，可看成为一个神经网络；参数化到输出层是一个变换（单变量基函数组合），也是一个神经网络。输出层的两个分量共享基函数。</p>
<p>隐层1的激活函数为双变量基函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image101.png" alt="img">；隐层3的激活函数为单变量基函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image102.png" alt="img">。中间隐层（隐层1、2、3）的输出节点所形成的向量均可分别看作为输入量在不同维数空间的“特征”。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image103.jpg" alt="img"></p>
<p>图10. 拟合曲线的神经网络。输入层（样本点）到参数化是一个神经网络变换；</p>
<p>参数化到输出层是一个神经网络变换。</p>
<p>类似地，拟合曲面也可以看作为一个如下图的多层神经网络。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image104.jpg" alt="img"></p>
<p>图11. 拟合曲面的神经网络。</p>
<p>从曲线曲面拟合的过程及神经网络结构来看，如果将整个网络进行优化，则拟合的过程是寻求样本点的参数化以及表达函数的基函数，使得拟合误差极小化。在这里，我们是知道给定样本点的“本征维数”，因此，人为设置了第一个隐层的维数为1（曲线）或2（曲面）。然而，在实际应用中，我们并不知道数据的本征维数，此时需要利用其他方法（比如流形学习或降维方法）来“检测”或“猜测”隐层的维数，或者直接根据经验来不断调参和测试。</p>
<p>需要提及的是，在计算机图形学中，传统的大部分方法是将参数化的问题和曲线曲面拟合问题分为两个问题来解决。有些工作专门做参数化，有些工作专门研究拟合问题。其中参数化的工作更为重要，直接决定了拟合曲线曲面的质量。此处不详述。</p>
<p>需要注意的是，本节只是从神经网络的观点来看曲线曲面的参数化和拟合的问题。在计算机图形学中，我们不是简单的通过上述网络来求参数化（单个样本点），而是要考虑更多的样本点一起来求解参数化。这个后面还会详细介绍。</p>
<p><strong>七、</strong>  <strong>通用人工神经网络</strong></p>
<p>有了上述的理解，我们现在来理解人工神经网络就非常容易了。</p>
<p>7.1.      函数拟合回顾</p>
<p>回顾前面所述的函数逼近论中的函数拟合问题，始终是存在着两个“纠结”。一是选择什么样的逼近函数类，即选择什么类型的基函数？二是选择多高的次数或阶数？</p>
<p>另一方面，如果从图8或图9的神经网络的观点来看，网络的结构设置都存在着如下两个“纠结”：</p>
<p>\1.   隐层中的节点中使用什么样的激活函数（基函数）？</p>
<p>\2.   隐层中设置多少个节点（基函数的次数）？</p>
<p>在传统逼近论中，上述两个问题就没有好方法来解决（第一个问题针对具体问题可预先设定），是通过不断试错来看结果的好坏来决定的。这是因为，虽然有些基函数的性质很好，但是次数或阶数过高（比如多项式基或三角函数基），就会产生震荡，也容易产生过拟合，使得拟合函数的性态不好。因此，一般不会采用次数或结束太高的基函数。为此，人们发展了分段拟合方法，即将数据分为若干段，每段分别拟合，各段的拟合函数保证一定的光滑性，称为样条函数。在逼近论和计算机图形学领域，人们发展出了很多漂亮的样条的理论与方法。</p>
<p>对于参数类型的高维曲线曲面，还存在着参数化的问题，这时还存在另一个纠结，就是要设置多少个隐层（图10和图11）？</p>
<p>7.2.      使用简单元函数作为激活函数</p>
<p>为了克服上述两个纠结的问题，是否可以通过其他形式来生成“基函数”？注意到，对于任意一个非常值的一元函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image105.png" alt="img">，这里我们称为元函数，其沿着<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image106.png" alt="img">方向的平移函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image107.png" alt="img">以及沿着<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image106.png" alt="img">方向的伸缩函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image108.png" alt="img">都与原函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image109.png" alt="img">线性无关。也就是说，如果能有足够多的变换所生成的函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image110.png" alt="img">，其线性组合所张成的函数空间就能有充分的表达能力。那么这些函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image111.png" alt="img">（可能都线性无关）是否能构成逼近一般函数的“基函数”呢？</p>
<p>一个自然的想法就是，我们能否以这个<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image105.png" alt="img">作为激活函数，让网络自动地去学习这些激活函数的变换<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image112.png" alt="img">，或者说去学习一些“基函数”，来表达所需要的拟合函数呢？这就是图12所示的神经元。变量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image106.png" alt="img">乘以一个伸缩，加上一个平移（称为偏置“bias”），即变量的仿射变换，成为神经元的输入<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image113.png" alt="img">；然后通过激活函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image114.png" alt="img">复合后成为该神经元的输出<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image115.png" alt="img">。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image116.jpg" alt="img"></p>
<p>图12. 一元（单变量）函数的神经元结构。</p>
<p>对于多变量的情形（多元函数），神经元的结构如图13所示。变量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image117.png" alt="img">的线性组合，加上一个平移（称为偏置“bias”），即变量的仿射变换，成为神经元的输入<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image118.png" alt="img">；然后通过激活函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image114.png" alt="img">复合后成为该神经元的输出<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image115.png" alt="img">。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image119.jpg" alt="img"></p>
<p>图13. 多元（多变量）函数的神经元结构。</p>
<p>不失一般性，我们下面就直接以多元函数的形式来介绍一般神经网络的结构。对于向量值函数（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image120.png" alt="img">是多维的），则分别将<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image120.png" alt="img">的各个维数分别看作一个多元函数处理而已，这些函数共享节点（基函数）即可。</p>
<p>7.3.      单隐层神经网络</p>
<p>一个多元函数的神经网络的结构如图14所示，有一个输入层，一个隐层及一个输出层。输入层除了变量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image117.png" alt="img">外，还有一个常数节点1；隐层包含多个节点，每个节点的激活函数都是<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image114.png" alt="img">，隐层的输出就是输入层节点的线性组合加偏置（即仿射变换）<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image121.png" alt="img">代入到激活函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image114.png" alt="img">的复合函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image122.png" alt="img">；输出层是这些复合函数的组合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image123.png" alt="img">。</p>
<p>这个网络的所有权系数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image124.png" alt="img">，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image125.png" alt="img">（输入层与隐层之间的权及偏置项）及<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image126.png" alt="img">（隐层与输出层之间的权，此层一般不用偏置项）作为这个神经网络的参数变量，需要通过极小化损失函数来求解的。这个过程称为“训练”或“学习”。</p>
<p>与前面所述的逼近论中的拟合函数类似，网络的隐层的节点数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">是需要通过不断调试和尝试来确定的。隐层的节点输出<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image127.png" alt="img">称为输入数据<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image128.png" alt="img">)的“特征”。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image129.jpg" alt="img"></p>
<p>图14. 多元（多变量）函数的单隐层神经网络结构。</p>
<p>整个网络的学习过程本质上就是在学习所有的系数参数。最后得到的拟合函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image123.png" alt="img">为一些函数的线性组合表达。这些组合函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image130.png" alt="img">实质上就是表达函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image131.png" alt="img">的“基函数”！</p>
<p>从函数逼近论的角度，我们可以这样来理解神经网络：神经网络的学习过程本质上是在学习基函数！这些基函数是通过激活函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image114.png" alt="img">通过平移和伸缩（变量的仿射变换）来得到的！</p>
<p>当然，天下没有免费的午餐。以前函数逼近论无法解决的两个“纠结”这里仍然存在：</p>
<p>第一个纠结是如何选取基函数？这里使用激活函数，然后学习其变换来得到。</p>
<p>第二个纠结是选择多少个基函数？这里为隐层的神经元个数。</p>
<p>从这个观点来看，神经网络本质上就是传统的逼近论中的逼近函数的一种推广。它不是通过指定的理论完备的基函数来表达函数的，而是通过简单的基元函数（激活函数）的不断变换得到的“基函数”来表达函数的。实质上是二层复合函数。</p>
<p>此时，我们当然要问个问题：将函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image105.png" alt="img">经过充分多的平移和伸缩（包括它们的组合<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image132.png" alt="img">）所线性张成的函数空间，其表达能力有多强？这个函数空间是否在所有函数空间是稠密的？如果问题是正确的，那么就是说，对于任何一个给定的函数，总能找到函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image105.png" alt="img">的多次平移和缩放的函数，其线性组合能够逼近给定的这个函数；也就是说，图14中的神经网络只要隐层的节点数足够多，该网络所表达的函数就能逼近任意的函数。</p>
<p>幸运的是，上述猜想在很多情况下是成立的！有以下的万能逼近定理所保证。</p>
<p>7.4.      万能逼近定理</p>
<p>记<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image133.png" alt="img">为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">空间中的单位立方体，我们在这个定义域中来描述万能逼近定理。记<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image134.png" alt="img">为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image135.png" alt="img">上的连续函数空间，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image136.png" alt="img">为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image135.png" alt="img">上的可测函数空间，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image137.png" alt="img">为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image135.png" alt="img">上相对测度<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image138.png" alt="img">的可积函数空间（即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image139.png" alt="img">）。</p>
<p>设给定一元激活函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image140.png" alt="img">，首先给出如下定义。</p>
<p>【定义7.1】称函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image141.png" alt="img">为压缩函数，如果<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image141.png" alt="img">单调不减，且满足</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image142.png" alt="img">, <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image143.png" alt="img"> .</p>
<p>【定义7.2】称函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image141.png" alt="img">为可分辨的，若对于有限测度<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image138.png" alt="img">，由</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image144.png" alt="img">,</p>
<p>可得到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image145.png" alt="img"></p>
<p>【定义7.3】记</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image146.png" alt="img"></p>
<p>为所有由激活函数变换及线性累加所构成的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image007.png" alt="img">维函数空间（即具有<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">个节点的单隐层神经网络所表达的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image007.png" alt="img">维函数）。</p>
<p>【定理7.1】若<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image147.png" alt="img">是压缩函数，则<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image148.png" alt="img">在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image134.png" alt="img">中一致稠密，在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image136.png" alt="img">中按如下距离<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image149.png" alt="img">下稠密：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image150.png" alt="img">.</p>
<p>【定理7.2】若<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image147.png" alt="img">是可分辨的，则<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image148.png" alt="img">在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image134.png" alt="img">中按连续函数距离下稠密。</p>
<p>【定理7.3】若<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image147.png" alt="img">是连续有界的非常值函数，则<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image148.png" alt="img">在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image134.png" alt="img">中稠密。</p>
<p>【定理7.4】若<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image147.png" alt="img">是无界的非常值函数，则<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image148.png" alt="img">在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image137.png" alt="img">中稠密。</p>
<p>上述定理这里就不详细解释，稍微有些实分析和泛函分析的基础就能看懂。用通俗的话来说，就是：对任意给定的一个<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">，只要项数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image022.png" alt="img">足够多，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image148.png" alt="img">中就存在一个函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">，使得<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image020.png" alt="img">在一定精度下逼近<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image021.png" alt="img">。也就是说，图14所表达的单隐层的神经网络所表达的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image007.png" alt="img">维函数能够逼近<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的任意一个函数。</p>
<p>根据上述定理可知，函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image147.png" alt="img">只要满足一定条件即可作为一个合适的激活函数。但还是存在着一个巨大的“纠结”，就是隐层中的节点数量该取多大，仍是未知的。在实际中，这个是要不断通过测试和调试来决定的，这个过程称为“调参”。</p>
<p>定理7.1-7.4的详细证明可见以下论文。</p>
<p>[7-1] K. Hornik, et al. Multilayer feedforward networks are universal approximations. Neural Networks, 2: 359-366, 1989.</p>
<p>[7-2] G. Cybenko. Approximation by superpositions of a sigmoidal function. Math. Control Signals System, 2: 303-314, 1989.</p>
<p>[7-3] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4: 251-257, 1991.</p>
<p>7.5.      深度神经网络：多隐层神经网络</p>
<p>将上述的神经网络的隐层进一步推广到多层，即可得到多隐层的神经网络，即深度神经网络（深度的意思就是多层）。其网络结构如图15所示。</p>
<p>由于每相邻的两层的节点都是全部连接的，因此在机器学习领域，也叫做全连接神经网络或多层感知机（Multi-Layer Perceptron, MLP）。</p>
<p>图15只是显示了<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image151.png" alt="img">的函数的神经网络的结构。对于一般的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image152.png" alt="img">的向量值函数，只要将最后输出层多几个节点即可，即，每个维度分量都是一个<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image151.png" alt="img">的函数，共享了前面的所有网络层结构。</p>
<p>每个隐层的输出<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image153.png" alt="img"> 都可以看成输入数据<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image154.png" alt="img">在不同维数空间<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image155.png" alt="img">下的 “特征”。</p>
<p>注意不要在输出层使用激活函数。因为最后的输出就是那些基函数的线性组合表达即可。</p>
<p>从数学形式上看，深度神经网络就是一个多层复合函数。可以证明（Kolmogorov–Arnold表示定理），任意一个多元函数可以表示成若干个单变量函数的复合（事实上，只需要三层网络即可）。这为使用深度神经网络用来逼近任意高维函数提供了理论基础。但遗憾的是，该定理只证明了复合函数逼近任意函数的存在性，但具体使用什么样的函数未可知。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image156.jpg" alt="img"></p>
<p>图15. 多元（多变量）函数的深度神经网络结构。</p>
<p>因此，深度神经网络也能很强地表达<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image064.png" alt="img">中的任意的函数。但在具体应用中，选用多少个隐层，每一隐层选用多少节点，这些都是需要根据损失函数的进行不断调试的。这个过程也称为“调参”。另外，从这个观点可以容易理解各种深度神经网络的工作机制，包括一般深度神经网络、卷积神经网络（CNN）、RBF神经网络、ELM等。</p>
<p><strong>八、</strong>  <strong>深入理解深度神经网络</strong></p>
<p>8.1.      激活函数</p>
<p>由7.4节中的万能逼近定理可知，激活函数的选择可以很多种。常用的激活函数如图16所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image157.jpg" alt="img"></p>
<p>图16. 常用的激活函数。</p>
<p>在早期，Sigmoid函数和tanh函数是人们经常使用的激活函数。近年来，随着神经网络的隐层不断增加后，人们偏向于使用ReLU函数作为激活函数，这是因为ReLU函数的在x轴正向的导数是1，不容易产生梯度消失问题（见8.3节）。这里值得一提的是，如果使用ReLU函数作为激活函数，则最后生成的拟合函数为分片线性函数。</p>
<p>关于激活函数的理解和讨论，机器学习领域及互联网上有很多文章进行过解释，都是解释为“激活函数的主要作用是提供网络的非线性建模能力，提供了分层的非线性映射学习能力”等等。我们从逼近论的角度，将非线性激活函数理解为拟合函数的“基函数”生成器（如果激活函数为线性的，则无法生成表达非线性函数的基！），训练神经网络就是在学习这些基函数，这是一种全新的观点，也是非常容易理解的。</p>
<p>8.2.      反向传播算法（BP优化算法）</p>
<p>从数学上看，深度神经网络就是一个多层复合函数。在机器学习中，这个复合函数的好坏依赖于对目标函数最大化或者最小化的过程，我们常常把最小化的目标函数称为损失函数（Loss function），它主要用于衡量机器学习模型（此处为深度神经网络）的预测能力。常见的损失函数有均方误差（L2误差）、平均绝对误差（L1误差）、分位数损失、交叉熵、对比损失等。损失函数的选取依赖于参数的数量、异常值、机器学习算法、梯度下降的效率、导数求取的难易和预测的置信度等若干方面。这里就不展开讨论。</p>
<p>给定了一批训练数据，计算神经网络的问题就是通过极小化损失函数来找到网络中的所有权值参数（包括偏置参数）。这个过程本质上就是利用这个神经网络函数来拟合这些数据，这个过程也称为“训练”或“学习”。</p>
<p>误差反向传播算法（Error Back Propagation，BP）是神经网络训练时采用的一种通用方法。本质是随机梯度下降法。最早见于如下论文：</p>
<p>[8-1] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by back-propagating errors. Nature, 323(99): 533-536, 1986.</p>
<p>由于整个网络是一个复合函数，因此，训练的过程就是不断地应用“复合函数求导”（链式法则）及“梯度下降法”。反向传播算法从神经网络的输出层开始，利用递推公式根据后一层的误差计算本层的误差，通过误差计算本层参数的梯度值，然后将差项传播到前一层。然后根据这些误差来更新这些系数参数。</p>
<p>值得一提的是，梯度下降法并不是唯一的优化方法。也有不少人使用其他优化方法，比如ADMM方法，来作为优化器优化深度神经网络的求解。</p>
<p>8.3.      什么是好的激活函数？</p>
<p>由万能逼近定理可知，只要网络规模设计得当，使用非线性函数作为激活函数（比如8.1节中常用的激活函数）的逼近能力都能够得到保证。</p>
<p>然后，由上一节的反向传播算法可知，算法过程中计算误差项时每一层都要乘以本层激活函数的导数，因此，会发生很多次的导数连乘。如果激活函数的导数的绝对值小于1，多次连乘之后误差项很快会衰减到接近于0；而参数的梯度值由误差项计算得到，从而导致前面层的权重梯度接近于0，参数不能得到有效更新，这称为“梯度消失”问题。与之相反，如果激活函数导数的绝对值大于1，多次连乘后权重值会趋向于非常大的数，这称为“梯度爆炸”。长期以来，这两个问题一直困扰神经网络层次无法变得很深。</p>
<p>最近几年，ReLU函数被经常使用作为深度神经网络的激活函数。有两个主要理由：</p>
<p>\1)   该函数的导数为sgn（忽略在0处不可导的情形），计算简单，在正半轴导数为1，有效的缓解了梯度消失问题；</p>
<p>\2)   虽然它是一个分段线性函数，但它具有非线性逼近能力；使用ReLU激活函数的深度网络的本质是用分段（分片）线性函数（超平面）去逼近目标。</p>
<p>后来，人们在ReLU的基础上又改进得到各种其他形式的激活函数，包括ELU、PReLU等。</p>
<p>8.4.      多个隐层的几何意义</p>
<p>现在我们已经理解了深度神经网络的本质，就是一个复杂的多层复合函数。训练的过程就是在求激活函数变换得到的“基函数”，因此，训练神经网络的本质就是在“学习基函数”。这跟稀疏学习中的“字典学习”非常类似。殊途同归！</p>
<p>我们来看一下深度神经网络所表达的拟合函数与传统所用的拟合函数（2.1节）的类比与区别：</p>
<p>(1)  传统的拟合函数是在某一函数类（由某些具有良好性质的基函数所张成的函数空间）中进行选择。基函数是预先给定的，个数也是预先设定的（或者稀疏选择的）。系统的求解变量是基函数组合系数；</p>
<p>(2)  神经网络所表达的拟合函数是由某一激活函数对变量的平移和伸缩变换而来，个数是预先设定的（神经元个数）。系统的求解变量是基函数的变换，因此可以看成是在学习基函数；</p>
<p>(3)  传统的拟合函数也是一种神经网络（见第6节），因此也可以拓展成深度网络（复合函数），但问题是次数或阶数经过复合后会变得非常高，更易造成过拟合；</p>
<p>(4)  多层神经网络中的不同层所用的激活函数也可以使用传统的基函数。因此，还可推广为更一般的深度神经网络。</p>
<p>那么为何要用多个隐层呢？事实上，通过6.3节的内容就不难理解了。每个隐层就是将前一层的数据作为输入（当然也包括输入层数据）在当前这个维数的空间中的一种映射，本质上就是在做“参数化”！</p>
<p>如果当前层的维数小于上一层的维数，就相当于将上一层的数据参数化到低维空间中。当然也可以参数化到高维空间，以提高自由度。</p>
<p>在6.3节中，对于三维曲面，我们已知它是二维流形，因此，我们知道将其映射到二维作为参数域就很自然了。因此有了图11这样的网络结构。</p>
<p>但对于一般的数据（比如人脸数据），是在背景空间（ambient space）上观测的，可能数据的维数非常高（比如1000x1000的图像按照像素个数的维数达到100万维）。但这些数据并不会充满整个高维空间，会具有低维的结构（称为本征维数），即数据落在一个低维子空间（或低维流形）上，并呈现一定的概率分布。从数据的角度来看，一个嵌入到高维空间的低维流形本质上就是一个参数曲面，参数域是那个低维空间。因此，将高维数据参数化到低维的空间是可行的。</p>
<p>这里也出现了两个纠结的问题。</p>
<p>第一，对于一个实际数据拟合问题，应该设置多少个隐层？</p>
<p>第二，每个隐层应该设置多少个节点（该隐层的维数）？</p>
<p>在机器学习领域，隐层的层数称为网络的深度，隐层的维数成为网络的宽度。对于基于深度神经网络的学习，到底是越深越好，还是越宽越好？</p>
<p>从数学上来看，网络越深，表示一个复杂的函数可以经过多次参数化的变换，映射到最终的目标。这个是合理的。举个例子，比如我们想将一个三维网格曲面参数化到一个平面，如果这个输入的曲面比较复杂，开口（参数化边界）很小，则要将曲面一次性展开（<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">的映射）比较困难。此时，可以多进行几次从<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">的映射，将曲面先在<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image084.png" alt="img">慢慢张开，每次张开的映射比较容易表达和计算。最后再映射到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">上就相对容易了，当然还可进行多次从<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">到<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image077.png" alt="img">的映射，生成更好的参数化。注意其中任何一个映射都可以是用一个神经网络来表达。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image158.jpg" alt="img"></p>
<p>图17. 带有特征的曲面的拟合。(a) 原始输入三维数据；(b) 为(a)的简单参数化（Floater参数化）；(c) 为优化过后的参数化；(d) 最后的拟合曲面。</p>
<p>在计算机图形学中，我们针对曲面展开及拟合问题，在如下的工作中用到了这种思想。</p>
<p>[8-2] Linlin Xu, Ruimin Wang, Zhouwang Yang, Jiansong Deng, Falai Chen, Ligang Liu. Surface Approximation via Sparse Representation and Parameterization Optimization. Computer-Aided Design (Proc. SPM), 78: 179-187, 2016.</p>
<p>如图17所示，(a) 为一个带有尖锐特征的输入点云，如果将其简单参数化为(b)，在其上构建的拟合曲面很难表现曲面的尖锐特征。这时我们对(b)中的参数化再做一个映射（优化）为(c)，这时在其上做的拟合曲面则能很好地表现出尖锐特征。注意，这里生成拟合曲面的基函数中我们采用了非光滑基函数（采用非光滑基函数来表现曲面特征的思想首次在我们的如下论文中所采用）。因此，上述工作本质上是一个使用5层神经网络（3个隐层）来优化拟合曲面。</p>
<p> [8-3] Ruimin Wang, Ligang Liu, Zhouwang Yang, Kang Wang, Wen Shan, Jiansong Deng, Falai Chen. Construction of Manifolds via Compatible Sparse Representations. ACM Transactions on Graphics, 35(2), Article 14: 1-10, 2016.</p>
<p>这里顺便提一下，在计算机图形学中，最近一些年有不少工作在做曲面参数化的优化工作（比如满足无翻转、低扭曲等约束）。从机器学习的观点来看，就是在做更多的参数映射的优化计算。当然，这些工作并不能简单地看成一个深度神经网络来做。这是因为，对参数化的问题，我们是知道曲面（所有数据）的整体结构的，而不只是一些离散的采样点。因此，我们在做参数化优化的时候，我们可以充分利用曲面的整体结构来得到一些几何度量来满足各种约束，而这些是神经网络优化所做不到的！</p>
<p>总之，对于第一个问题，隐层越多，表示网络越深，确实会使得最后的映射带来更好的表现。其数学本质是将输入数据不断在不同的空间中进行参数化映射。最后得到的映射（可能非常复杂的映射）表达为若干个相对简单映射的复合。对于多隐层的解释和讨论，机器学习领域及网上有很多文章进行过解释，大部分都解释为“提取数据的特征”等等。我们从逼近论的角度（结合三维曲面拟合的例子），将多隐层优化看成数据在不同维数空间的参数化，是非常容易直观理解的。</p>
<p>对第二个问题，每个隐层的具体的节点数或维数，该如何设置呢？对于一般的数据这个就很难直观确定了，一般要根据经验。当然，也有一些办法来进行指导，我们将在下一节中理解。</p>
<p>8.5.      流形学习（数据降维）简介</p>
<p>对于一般的数据，我们并不知道其本征维数（即参数域空间），那么映射（参数化）到多少维空间是合适的呢？</p>
<p>事实上，这个也是个非常纠结的问题！这个问题就是寻求高维数据的本征维数，在机器学习领域称为流形学习（Manifold learning），或降维问题（Dimension reduction）。如图18所示，数据是在三维空间给出的（B），每个点是3个坐标，看起来是三维数据；但其本质上位于三维空间的一张曲面上，即二维流形曲面上（A），其本征维数是2。因此，可以将其一一映射到平面上（C）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image159.jpg" alt="img"></p>
<p>图18. 流行学习。（图片来自论文[8-5]）</p>
<p>提到流形学习，不能不提到如下的两篇开山之作（同时发在Science 2000的文章）：</p>
<p>[8-4] Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500). 2000: 2323-2326.</p>
<p>[8-5] Tenenbaum, Joshua B and De Silva, Vin and Langford, John C. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500). 2000: 2319-2323.</p>
<p>流形学习在本世纪初的前一个十年是一个研究热点（由上面两篇Science论文带起来的）。至今已发展出了许多方法，从简单的线性方法，比如PCA；到后面的非线性方法，比如局部线性嵌入（LLE），等距映射法（Isomap），Laplace特征映射，局部保投影法（LPP）等。这里不详细介绍。</p>
<p>需要指出的是，正如第6节所讲的三维曲面参数化的计算，我们计算数据的本征维数的时候，是需要利用所有数据的信息及其几何或拓扑结构一起来分析的，而不能像神经网络训练那样，仅仅将数据灌到神经网络中去更新权系数。</p>
<p>8.6.      端对端学习（End-to-end learning）</p>
<p>在传统的机器学习中，很多任务的输入数据的维数都非常大，不易直接处理。往往需要先降到合适的维数，然后再选择合适的预测模型来做拟合。</p>
<p>传统机器学习的流程一般由两个主要的模块组成。第一个模块称为特征工程，将原始输入数据（高维向量）变换为一个低维向量，即，用这个低维向量来作为表达输入数据的特征（或描述子，feature or descriptor）。不同的人有不同的方法来计算这个低维特征，称为特征抽取（Feature extraction），如图19显示了不同的图像特征抽取方法。各种计算的特征都有一定的实用范围，因此，有人就提出如何选择或组合各种特征得到更好的特征，这是特征工程的另一个子问题，称为特征选择（Feature selection）。针对三维形状分割，我们发现对三维局部块的众多描述子并不是串联起来一起使用更好，而是针对不同的形状的部位使用不同的描述子会更好，于是我们使用稀疏选择的方法来选择形状描述子用于形状分割，详细可见如下论文。</p>
<p>[8-6] Ruizhen Hu, Lubin Fan, Ligang Liu. Co-Segmentation of 3D Shapes via Subspace Clustering. Computer Graphics Forum (Proc. Symposium on Geometry Processing), 31(5): 1703-1713, 2012.</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image160.jpg" alt="1365435491_6508.jpg"></p>
<p>图19. 图像的各种特征抽取方法。</p>
<p>由于这种特征是通过人为设计的算法来得到的，因此这种特征称为人工特征（Hand-crafted features）。人工特征的抽取需要人们对输入数据的认知或者领域知识（Domain knowledge），因此在很多情况下会局限于人的经验和认知。</p>
<p>第二个模块称为预测模型，即选用什么样的预测方法来拟合给定的数据，我们前面所讲的拟合就是指这个过程。预测模型的目标就是希望学习到的模型在预测未知目标时越精确越好。一般要考虑模型的复杂度及精确度等因素。在机器学习领域有非常多的方法，这里不一一介绍。</p>
<p>上述两个模块中的每个模块都是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端的，如图20（上）所示。</p>
<p>特征提取的好坏异常关键，甚至比预测模型还重要。举个例子，对一系列人的数据分类，分类结果是性别（男或女），如果你提取的特征是头发的颜色，无论分类算法如何，分类效果都不会好；如果你提取的特征是头发的长短，这个特征就会好很多，但是还是会有错误；如果你提取了一个超强特征，比如染色体的数据，那你的分类基本就不会错了。</p>
<p>这就意味着，特征需要足够的经验去设计，这在数据量越来越大的情况下也越来越困难。于是就出现了端到端的网络，特征可以自己去学习。所以特征提取这一步也就融入到算法当中，不需要人来干预了。即不需要将任务分为多个步骤分步去解决，而是从输入端的数据直接得到输出端的结果。前面介绍的深度神经网络就是一个端到端的学习方法。如图20（下）所示。</p>
<p>端到端学习的好处在于，使学习模型从原始输入到最终输出，直接让数据“说话”，不需人工设计的模块；给模型更多可以根据数据自动调节的空间，增加模型的整体契合度。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image161.jpg" alt="img"></p>
<p>图20. 非端到端的学习（上）与端到端的学习（下）。</p>
<p>但端到端学习也有不足。第一，端到端的深度神经网络需要大量的样本数据才能工作得很好，因此很多公司都在花费大量的人力物力去生成标注数据（一般通过人工标注，催生了很多专门做数据标注的外包公司）；第二，人工设计的特征并不是都不能用的，有些人工特征还是能代表着人类智能的，是可以用来作为合理的特征的，而端到端的学习无法用这些特征。当然，通过适当的改造和结合，也可以集成人工特征到深度神经网络中，这里就不展开讨论。</p>
<p>8.7.      深度学习成功的原因</p>
<p>基于深度神经网络的端到端学习在最近几年取得很大的成功，被大量应用与计算机视觉、语音识别、自然语音处理、医学图像处理等领域中。从数学本质上来看，神经网络就是一个映射函数而已，在上世纪50年代就有了感知机，但为什么以前没有火呢？ 主要有以下的两方面的原因。</p>
<p>第一，之前没有那么大规模的数据量；</p>
<p>第二，以前的工程技术（先进优化算法及支持大规模并行计算的GPU硬件）无法求解的很深的神经网络。</p>
<p>因此，在上个世纪90年代，各种浅层模型大行其道，比如只有一层隐层的支撑向量机（Support Vector Machine，SVM）以及没有隐层的逻辑回归方法（Logistic Regression，LR）等。</p>
<p>直到最近几年，随着各种传感器的大量使用以及移动互联网的广泛应用，能够获取和产生海量（PB级甚至ZB级）的数据（比如文本、图像、语音等）。特别是ImageNet图像数据库的诞生以及基于该数据库的各种竞赛，直接将计算机视觉领域中的深度学习的研究与应用推向了“落地”，催生了大量的计算机视觉的人工智能公司。</p>
<p>8.8.      过拟合</p>
<p>自从深度学习火了后，各种深度神经网络（或CNN网络）随之产生了，比如2012年的AlexNet有8层；2014年的VGG-19有19层；2014年的GoogleNet有22层；2015年的ResNet有152层！甚至还出现了上千层的深度神经网络！</p>
<p>从数学上可知，拟合函数所带的参数的个数与样本数据的个数之间的差代表着这个拟合函数的“自由度”。网络越来越“深”后，拟合模型中的可调整参数的数量就非常大，通常能达到百万计甚至几十亿级。因此，层数很大的深度网络（模型过于复杂）能够表达一个自由度非常大的函数空间，甚至远高于目标函数空间（过完备空间），即自由度远大于0。这样就很容易导致过拟合（Overfitting），如前面所讨论过的图2（右）所示。</p>
<p>过拟合可以使得拟合模型能够插值所有样本数据（拟合误差为0！）。但拟合误差为0不代表模型就是好的，因为模型只在训练集上表现好；由于模型拟合了训练样本数据中的噪声，使得它在测试集上表现可能不好，泛化性能差。为此，人们采取了不同的方法来缓解过拟合（无法完全避免），比如正则化、数据增广、Dropout、网络剪枝等。</p>
<p>在使用深度神经网络来做深度学习的应用中，很多工作都是直接使用现有的深度神经网络，或者改造现有的深度神经网络。如果网络设计不够好，大部分结果都可能有过拟合的现象。</p>
<p>在实际应用中，我们可以挑选样本数据中的一部分作为训练数据集，其他的作为测试数据集。如果网络在训练数据集上表现好（即损失函数很小），而在测试数据集上表现不够好，那么就可能发生了过拟合，此时就适当减少网络的层数或者节点数。这个过程是需要有耐心，可能需要花费很长时间才能调到一个合适的网络。需要靠感觉、经验和运气。因此，有人戏称调试深度神经网络的过程为“炼丹”。</p>
<p>8.9.      类比神经科学的解释</p>
<p>我们已从函数逼近论的角度来理解了人工神经网络本质是传统逼近函数的一个推广，由人工选取基函数到自动学习基函数，由人工提取特征到自动计算特征（参数化），能够较容易理解人工神经网络学习是如何工作的。</p>
<p>在机器学习领域，人们从生物神经网络的角度来解释和理解人工神经网络的工作原理，将人工神经网络看成为一个可以模拟人脑工作行为的函数。</p>
<p>在生物神经科学（Neuroscience，包括脑科学、认知神经科学等）的研究中，人们发现，人体的神经元通过树突接受信号。这些信息或信号随后被传递到脑细胞或细胞体。在细胞体内部，所有的信息将被加工生成一个输出。当该输出结果达到某一阈值时，神经元就会兴奋，并通过轴突传递信息，然后通过突触传递到其他相连的神经元。神经元间传输的信号量取决于连接的强度。如图21（左）。</p>
<p>我们来类比人工神经网络：如果把树突想象成人工智能网络中基于突触感知器的被赋予了权重的输入。然后，该输入在人工智能网络的“细胞体”（神经元）中相加。如果得出的输出值大于阈值单元，那么神经元就会“兴奋”（激活），并将输出传递到其它神经元，如图21（右）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image162.jpg" alt="img"></p>
<p>图21. 左：人脑神经元结构；右：从生物神经网络类比得出的人工神经元结构。</p>
<p>（图片来自互联网）</p>
<p>在计算机视觉领域，人们也从视觉神经科学（Visual Neuroscience）中类比来解释了在计算机视觉领域用得最多的卷积神经网络（CNN）的工作机理。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image163.jpg" alt="img"></p>
<p>图22. 视觉皮层结构。（图片来自互联网）</p>
<p>由视觉机理的研究发现，动物大脑的视觉皮层具有分层结构。眼睛将看到的景象成像在视网膜上，视网膜把光学信号转换成电信号，传递到大脑的视觉皮层（Visual cortex），视觉皮层是大脑中负责处理视觉信号的部分。1959年，有科学家进行了一次实验，他们在猫的大脑初级视觉皮层内插入电极，在猫的眼前展示各种形状、空间位置、角度的光带，然后测量猫大脑神经元放出的电信号。实验发现，当光带处于某一位置和角度时，电信号最为强烈；不同的神经元对各种空间位置和方向偏好不同。这一成果后来让他们获得了诺贝尔奖。</p>
<p>目前已经证明，视觉皮层具有层次结构。从视网膜传来的信号首先到达初级视觉皮层（primary visual cortex），即V1皮层。V1皮层简单神经元对一些细节、特定方向的图像信号敏感。V1皮层处理之后，将信号传导到V2皮层。V2皮层将边缘和轮廓信息表示成简单形状，然后由V4皮层中的神经元进行处理，它颜色信息敏感。复杂物体最终在IT皮层（inferior temporal cortex）被表示出来。</p>
<p>卷积神经网络可以看成是上面这种机制的简单模仿。它由多个卷积层构成，每个卷积层包含多个卷积核，用这些卷积核从左向右、从上往下依次扫描整个图像，得到称为特征图（feature map）的输出数据。网络前面的卷积层捕捉图像局部、细节信息，有小的感受野，即输出图像的每个像素只利用输入图像很小的一个范围。后面的卷积层感受野逐层加大，用于捕获图像更复杂，更抽象的信息。经过多个卷积层的运算，最后得到图像在各个不同尺度的抽象表示（由底层到高层的特征表达），如图23所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image164.jpg" alt="img"></p>
<p>图23. 基于卷积神经网络的深度学习所得到的人脸图像的分层特征。</p>
<p>（图片来自于Stanford大学的论文）</p>
<p>笔者认为，利用神经科学的神经元或神经网络来解释深度学习中的人工神经网络，这仅仅是类比而已。从我们的分析可知，深度学习中的人工神经网络并没有真正的认知和学习！如果我们人类对自己的认知系统无法完全了解的话，我们是很难发展出真正的人工智能技术。</p>
<p>8.10.    深度人工神经网络的局限：保持结构的映射</p>
<p>我们从数学上理解了深度人工神经网络本质上是表达不同维数空间之间的一个映射（函数）。给了一些样本点后，通过调整网络的参数权系数，不断极小化损失函数，从而达到最好的拟合结果得到了拟合函数。</p>
<p>这个拟合过程只极小化了逐点的误差或者累积逐点误差，而无法考虑点与周围点之间的其他信息（比如邻域几何结构、局部几何度量、流形的整体拓扑结构等）。如果要考虑点之间的几何与拓扑的结构信息，则深度人工神经网络就无能为力（当然一定要改造也是可以的），这时就需要其他的数学方法了。有点像数学分析中，逐点收敛和一致收敛，逐点连续和一致连续的区别。</p>
<p>例如，前面介绍的流形学习中，LLE方法保持每个样本点与它相邻的多个点的线性组合（体现了局部线性）来重构低维空间的点的分布，相当于用分段的线性面片近似代替复杂的几何形状，样本投影到低维空间保持这种线性重构关系，如图24（左）所示。Isomap方法希望保持数据在低维空间的映射之后能够保持流形上的测地线距离，即全局的几何结构，如图24（右）所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image165.jpg" alt="img">  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image166.jpg" alt="img"></p>
<p>图24. 流形学习。左：保持局部的几何信息（LLE）；右：保持全局的距离信息（Isomap）。</p>
<p>（图片分别来自于论文[8-4]和[8-5]）</p>
<p>再如，在计算机图形学和几何处理领域，我们做的很多映射（比如三维网格曲面的参数化），也需要保持曲面上的某些几何性质（保角度、保面积、保形等），也发展出许多不同的求解映射的方法和技巧。例如，以下两个工作是我们做的有关三维曲面参数化到二维平面的工作，第一个工作是曲面参数化工作的经典方法之一，已经被集成到3D System公司的建模软件以及著名的计算几何算法<a target="_blank" rel="noopener" href="https://www.cgal.org/">CGAL库</a>（The Computational Geometry Algorithms Library）中。第二个工作是今年我们最新的一个工作，能快速生成无翻转和低扭曲的网格参数化，论文即将在下周的Siggraph 2018会议上汇报。</p>
<p>[8-7] Ligang Liu, Lei Zhang, Yin Xu, Craig Gotsman, Steven J. Gortler. A Local/Global Approach to Mesh Parameterization. Computer Graphics Forum (Proc. Eurographics Symposium on Geometry Processing (SGP)), 27(5), 1495-1504, 2008.</p>
<p>[8-8] Ligang Liu, Chunyang Ye, Ruiqi Ni, Xiao-Ming Fu. Progressive Parameterizations. ACM Transactions on Graphics (Proc. Siggraph), 37(4), Article 41: 1-12, 2018.</p>
<p>在计算机图形学中，有时我们也需要将三维空间的二维流形曲面映射到另一个规则二维流形曲面，比如球面上，这时称为球面参数化。如读者有兴趣可阅读我们如下的3篇有关球面参数化的工作。</p>
<p>[8-9] Xin Hu, Xiao-Ming Fu, Ligang Liu. Advanced Hierarchical Spherical Parameterizations. IEEE Transactions on Visualization and Computer Graphics, to appear, 2018.</p>
<p>[8-10] Chunxue Wang, Xin Hu, Xiaoming Fu, Ligang Liu. Bijective Spherical Parametrization with Low Distortion. Computers&amp;Graphics (Proc. SMI), 58: 161-171, 2016.</p>
<p>[8-11] Chunxue Wang, Zheng Liu, Ligang Liu. As-Rigid-As-Possible Spherical Parameterization. Graphical Models (Proc. GMP), 76(5): 457-467, 2014.</p>
<p>另外，在计算机图形学及计算机辅助几何设计领域，我们还有独特的曲线和函数的构造方法，如通过Bernstein基函数构造的Bezier曲线曲面，通过B样条基构造的B样条曲线曲面（分段Bezier曲线曲面），以及更一般的非均有有理B样条曲线曲面，通过控制顶点就能很直观控制曲线曲面的形状，而且发展出非常完备的理论及设计方法，如图25所示。这些独特的曲线曲面的构造方法也是我们领域独特的，已成为现代工业中产品曲面造型的工业标准（标准数学表达）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image167.jpg" alt="img"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image168.jpg" alt="img"></p>
<p>图25. NURBS曲线曲面：产品曲面造型的工业标准。（图片来源与互联网）</p>
<p><strong>九、</strong>  <strong>实战深度神经网络学习</strong></p>
<p>由于本文是面向非机器学习专业介绍深度学习的，因此，笔者在这里只是大致介绍一下各种基于深度神经网络的学习方法的实际技巧，不详细展开讨论，有兴趣的读者可查阅更专业的书籍或论文。</p>
<p>9.1.      看懂各种深度神经网络结构</p>
<p>从数学上来看，深度神经网络仅仅是一种函数的表达形式，是复杂的多层复合函数。由于它有大量的可调参数，而且近年来随着大数据、优化算法和并行计算GPU硬件的发展，使得用大规模的神经网络来逼近和拟合大数据成为可能。至今，不同领域的研究工作者及产业界工作者已将深度神经网络应用到各行各业的各个问题，而且提出了各种形式、各种结构的深度网络结构，使人眼花缭乱，似乎难以捉摸。但事实并非如此！</p>
<p>万变不离其宗，神经网络就是一个多层复合函数。网络结构发生改变，就是说函数的复合形式发生了变化。因此，只要掌握了这条原则，各种复杂的神经网络你就能轻松看懂。亦可参考互联网上的“<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35082030/article/details/73368962">一文看懂25个神经网络模型</a>”一文。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image169.jpg" alt="img"></p>
<p>图26. 多层神经网络所表达的函数：可以看成前面的若干层网络所表达的函数（蓝色）与后面若干层网络所表达的函数（红色）的复合。</p>
<p>简单地说，看懂各种神经网络结构只需看懂如下3个要素（具体解释就不详细展开了）：</p>
<p>(1)  神经元与激活函数：神经元接收什么数据，通过什么样的激活函数输出数据。人们发明了许多不同种类的神经元，比如，池化神经元和插值神经元（Pooling and interpolating cells）、概率神经元（Mean and standard deviation cells）、循环神经元（Recurrent cells ）、长短期记忆神经元（Long short term memory cells）、门控循环神经元（Gated recurrent cells）等。不管什么样形式和名称的神经元，我们只要搞明白神经元是通过什么样的函数来将接收数据变为输出数据，不同之处就在接收数据和激活函数不同而已，如图21（右）所示。</p>
<p>(2)  神经网络结构：不同的神经网络就是不同的多层复合形式的函数。比如，卷积神经网络（Convolutional Neural Network，CNN）就是一种限于局部感受野来改造的神经网络（线性组合特殊化为局部区域的卷积操作，且增加了池化(Pooling)层），特别适合于文本、图像及视频类数据的深度学习。再比如，残差神经网络（Residual Neural Network，ResNet）有一种特殊的连接，可以把数据从某一隐层传至后面几层（通常是2到5层）。该网络的目的不是要找输入数据与输出数据之间的映射，而是致力于构建输入数据与输出数据+输入数据之间的映射函数。本质上，ResNet 是在学习映射函数的一阶差分（一阶导数），能够有效抑制优化过程中的梯度消失。类似地， DenseNet (CVPR 2017) 实质上是在学习映射函数的高阶差分，因此也能有效抑制梯度消失现象。同样地，其他的各种神经网络结构也很容易看懂的，比如 AutoEncoder，GAN，Variational GAN等。</p>
<p>(3)  神经网络的组合：一个多层的神经网络就是一个多次复合的函数。中间的任何一层的输出都可以看成为原始输入数据的一个特征。如图26所示，一个多层神经网络所表达的函数：可以看成前面的若干层网络所表达的函数（蓝色）与后面若干层网络所表达的函数（红色）的复合。因此，在任何的一个中间层，可以修改输出的特征向量、可以插入一个其他的神经网络（将该特征向量作为输入，或与其他网络的特征向量进行叠加、运算等）等等。如果将神经网络看成为积木，整个组合过程就像在“搭积木”。这样就可以发明和产生无穷无尽的网络结构（比如，并行网络、累加网络等等很多网络名称）。在实际应用中，需要根据问题本身的特点来合理设计网络的结构。具体哪种网络结构合理，有无最好，都是组要根据问题本身思考和试验的，主要是要看损失函数的误差以及在应用中的效果来决定的。这个我们在后面还会进一步讨论。</p>
<p>9.2.      如何调试深度神经网络？</p>
<p>一个深度神经网络所表达的函数具有成千上万个参数，为了让损失函数在训练数据上达到最小，就需要设计合理的网络来拟合这些数据。在优化的过程中，要保证迭代收敛也是必要的。</p>
<p>一般地，调试较大的深度神经网络所花费的时间和劳力比调试传统程序要多，而且要有耐心！因为如果最后的结果不好（损失函数很大或者不收敛），如果确认你的代码无bug，那么原因只有一个，就是你所调的网络还不够好(orz)！</p>
<p>在调试神经网络的过程中，有太多不确定的因素需要你去仔细思考并小心翼翼地去调试，而这些大部分都需要靠直觉、经验和少许运气来调试决定的，比如：</p>
<p>(1)  需要选取多大的神经网络（也就是选用什么样的拟合函数）？具体地，网络要多少层？每层多少节点？这个只能凭直觉或经验了。简单点的话，就套用现成的网络结构，或者再经过一些修改即可。从函数的自由度来看，网络的参数个数与数据大小不应相差太大。</p>
<p>(2)  初始化。对于高度非线性的函数的最优化，一个非常重要的因素就是如何挑选一个好的初始化？</p>
<p>(3)  在优化过程中，如何判断优化过程是否收敛？</p>
<p>(4)  是否发生了过拟合？</p>
<p>(5)  … ….</p>
<p>以下有些简单的小经验分享下：</p>
<p>(1)  一定要可视化你的结果！这样有助于你在训练过程中发现问题。你应该明确的看到这些数据：损失函数的变化曲线、权重直方图、变量的梯度等。不能只看数值，一定要可视化！</p>
<p>(2)  网络不是越深越好！不要一上来就用VGG19, ResNet50等标准的大网络，可能你的问题只要几层就能解决问题。</p>
<p>(3)  先建立一个较小的网络来解决核心问题，然后一步一步扩展到全局问题。</p>
<p>(4)  先用小样本做训练，有了感觉后，再调试大的训练数据。</p>
<p>(5)  如果在几百次迭代后，迭代还没开始收敛，那么就要终止迭代，不要傻等了！要考虑修改你的网络了。</p>
<p>(6)  注意避免梯度消失或梯度爆炸。</p>
<p>(7)  另外，在互联网上还能找到许多网友们分（tu3）享（cao2）的实用调网络的经验，读者可自行去查找学习。</p>
<p>值得提醒的是，上述分享的各个经验没有绝对的对与错，须将它们当作经验去学习和使用即可。需要根据你自己的实际问题来设计和调试你的深度神经网络。</p>
<p>即便对于行家来说，调试较大的神经网络也是一项艰巨的任务。数百万个参数在一起决定的函数，一个微小的变化就能毁掉所有辛勤工作的成果。然而不进行调试以及可视化，一切就只能靠运气，最后可能浪费掉大把的青春岁月。</p>
<p>正因为这些原因，调试深度神经网络被认为是个不可解释的“黑箱子”，优化网络的过程成为“调参”，也被戏称为“炼丹”，是个需要丰富经验的“技术活”。</p>
<p>9.3.      深度学习开发工具</p>
<p>要学习和使用基于深度神经网络的深度学习，门槛并不高。你大不必从头开始自己搭建神经网络系统和实现优化算法。近几年来，很多大公司和科研机构都在研究自己的深度学习框架，且推出了不少深度学习开发平台（如图27），深度学习的开发框架和工具也越来越多，使得应用深度学习的入门门槛越来越低。使用好的开发工具，可让使用者减少底层开发的工作量，而将重点关注于深度学习应用逻辑的开发及模型的优化上，提高开发效率。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image170.jpg" alt="img"></p>
<p>图27. 基于深度神经网络的深度学习的开发工具。（图片来自互联网）</p>
<p>这里简单介绍几款大家常用的深度学习开发工具包，基本都是开源的。</p>
<p>(1)  TensorFlow, 由Google公司发布。支持多CPU及多GPU并行化运行，并支持CNN，RNN等主要的深度学习模型。Github社区人气最火的深度学习开源项目。</p>
<p>(2)  Caffe，由加拿大Berkeley BVLC实验室发布。使用最广泛的深度学习工具之一，提供C++，Python，Matlab等语言接口。</p>
<p>(3)  Torch，基于 Lua 脚本语言的工具，支持 iOS、 Android 等嵌入式平台。</p>
<p>(4)  Theano，基于Python语言开发的深度学习开源仿真平台。</p>
<p>(5)  Keras，基于Python语言开发的，底层库使用Theano或TensorFlow。其模块化特性非常适合刚入门的初学者快速实验并测试深度学习网络的性能，同时也开放提供对底层的修改。</p>
<p>关于这些开发工具包的使用，互联网上（包括Github）有非常丰富的教材和使用文档，读者有兴趣可以自行查阅学习。</p>
<p>9.4.      深度神经网络专用芯片</p>
<p>深度神经网络专用芯片（使用GPU+DNN的专用芯片）是一种特殊用途的处理器，它能够使得深度神经网络的训练更快，具有更高的性能和更低的功耗。DNN 专用芯片在性能和能源消耗方面的好处是显著的，DNN专用芯片的广泛使用还需要神经网络体系结构的标准化和对不同DNN框架的支持。国外的一些公司，包括Google、Nvidia、Intel等公司，已经开发和部署了DNN专用芯片，为基于DNN的应用程序提供了极高的性能。国内也有不少公司（如寒武纪、阿里巴巴等）在从事DNN专用芯片的研发，也已经达到国际前沿水平。</p>
<p><strong>十、</strong>  <strong>深度神经网络的能力与局限</strong></p>
<p>深度学习真有智能吗？遗憾的是，没有！</p>
<p>10.1.    基于深度神经网络的深度学习的本质再回顾</p>
<p>在本文，笔者从函数逼近论的角度已详细剖析了基于深度神经网络的深度学习，归纳为以下4个基本点：</p>
<p>(1)  深度神经网络（DNN）是一个复合函数。对于任何单变量函数，只要神经元足够多，神经网络函数就能任意逼近它；对于任何多变量函数，一定可以被多个单变量函数的复合来逼近，因此就能被深度神经网络函数进行逼近；上述两点为神经网络函数的表达能力提供了理论基础；</p>
<p>(2)  基于DNN的深度学习是个数据拟合（最小二乘回归）的过程；</p>
<p>(3)  拟合过程是在学习基函数；</p>
<p>(4)  多层映射是在做多次变换（或参数化）。</p>
<p>因此，基于DNN的深度学习是可以很清楚被理解的！具体的应用过程中，选多少层、选多少基的问题在逼近论中就是没有很好的解决方案，这是逼近论中就存在的问题，并不是深度网络带来的问题！</p>
<p>从数学本质来看，深度学习是在做最小二乘回归；其他的机器学习方法也是基于最小二乘回归的其他回归方法或者统计方法。</p>
<p>从应用性来看，还有以下一些对基于深度神经网络的深度学习的本质的说明：</p>
<p>(1)  基于DNN的深度学习实际上在做拟合和统计，网络结构设计需要靠直觉、技巧和经验以及运气；</p>
<p>(2)  激活函数和损失函数的选择也很重要；</p>
<p>(3)  DNN的层数、各层的神经元个数决定了该函数的参数；一般的DNN都具有非常多的参数；</p>
<p>(4)  DNN适合拟合大规模数据；对小规模数据，不建议用DNN；</p>
<p>(5)  任何一个中间隐层的输出向量都可以作为输入数据的特征（在该维数上的），也是输入数据的参数化；</p>
<p>(6)  DNN可以任意拼装、组合成各种复杂的网络结构；</p>
<p>(7)  大部分论文中的DNN结果都可能是过拟合；因此，很容易出现误判的情况（输入的轻微变化，比如给图片加少量噪声，结果就相差很大），容易犯大错；</p>
<p>(8)  DNN是以成败论英雄的，需要在实际应用中好才是真的好，而这个永远不知道。</p>
<p>需要再提一下的是，本文所讨论的基于DNN的深度学习只是深度学习的一种方法而已。还有其他的深度学习，比如南京大学周志华老师团队所提的深度森林等方法。</p>
<p>10.2.    深度神经网络具有“智能”吗？</p>
<p>从上面的剖析来看，基于DNN的深度学习实质上就是对样本数据集的拟合而已！显而易见，它仅仅是一个由数据驱动的产生的拟合器或分类器，根本不是一个“认知系统”！我们并没有看出来它有任何“智能”（即对数据所呈现的规律、知识的理解）的存在！对人工神经网络的研究可能仍然是几十年前的进展，只不过现在网络的规模变大了（数据大了，优化强了）而已！</p>
<p>从逼近论的角度来看，如果数据足够大，多到足以（稠密）覆盖整个目标函数，那么简单的最近邻查找或者局部线性插值的性能就足够好了。此时，笔者猜测，深度学习的性能与最近邻查找的性能可能相差无几！</p>
<p>举个例子，市面上出现了许多种类的所谓的“智能机器人”，声称可以与人进行自由对话。建议读者去测试一下这些对话机器人，你们就会发现，这些机器人的问答系统基本没有理解（只有少数有少量简单的理解）。如果对于在其数据库中存在的问题或类似的问题，机器人能够回答得比较合理；如果遇上需要逻辑推理的问题，这些机器人基本就答非所问，或者王顾左右而言他，甚至开始“卖萌”了。有空去逗逗这些对话机器人也是蛮有意思的事。</p>
<p>对于自动翻译等机器人，只要有足够的两种语言的对应词条或句子（比如有些公司的词库数据库非常庞大，达到亿级），就能“翻译”（拟合）得还不错，达到一定的实用水准。但从本质上来讲，这些翻译算法并没有真正理解语言本身！</p>
<p>关于深度学习，我们要有清醒的认识：</p>
<p>\1)   深度学习模型需要大量的训练数据，才能有较好的效果；因为这些学习模型都在做数据拟合或回归，或者是在做统计；</p>
<p>\2)   实际问题中，往往会遇到小样本问题，采用传统的简单的机器学习方法，则可以很好地解决了，没必要非得用复杂的深度学习方法；</p>
<p>\3)   神经网络看起来来源于人脑神经元的启发，但绝不是人脑的模拟！它只是一个复合函数！举个例子，给一个三四岁的小孩看过几只猫之后，他就能识别其他的猫。也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度神经网络的学习方法显然不是对人脑的模拟。</p>
<p>这几年各种媒体的大量夸张的报道和宣传（即使是战胜人类的围棋算法AlphaGo，也并不是真正的人工智能，而仅仅是机器在一定的游戏规则下的策略计算能力），极度夸大了人工智能的进展，让很多大众误认为“强人工智能”已经来临，甚至产生不必要的恐慌。</p>
<p>事实上，现在的人工智能的进展仍然很小，正如清华大学人工智能研究院院长张钹院士在2018年全球人工智能与机器人峰会（CCF-GAIR）上的报告上所指出的：“人工智能刚刚起步，离真正的人工智能还很遥远… 我们必须走向具有理解的人工智能，这才是真正的人工智能。”</p>
<p>10.3.    深度学习在工业界的应用</p>
<p>虽然近几年发展起来的基于DNN的深度学习并未产生真正的智能。但由于现在有大量的数据，基于数据驱动的深度学习方法可以在很多领域确实得到了落地的应用。特别是在计算机视觉领域，比如人脸识别、图像分割与识别等，该领域做了很多年的一些问题的研究工作，瞬间就被深度学习所击败！深度学习取得了巨大的成功！因此，我们仍能使用深度学习解决不少问题。</p>
<p>近几年，催生了大量的人工智能公司，分布在安防、医疗、金融、智能机器人、自动驾驶、智能芯片、智能家居等领域；各高校也纷纷构建了人工智能研究院或大数据学院等；各学会成立了人工智能相关的专业委员会；国家也纷纷出台了有关人工智能的政策与文件，包括《新一代人工智能发展规划》，鼓励人工智能方面的人才培养（从娃娃抓起）和产业落地。在科研领域，大量的研究工作者和学生都涌进到人工智能领域，在一些人工智能或计算机视觉领域的会议的投稿量迅速增加，参加会议的人数达到历史新高。</p>
<p>但残酷的事实是，绝大部分的人工智能公司都是“伪人工智能”，他们或者仅仅在做着数据标注的工作，因为标注数据的多少决定着拟合函数的“智能”！另外，已经有些公司因未能找到合适的盈利模式而倒闭。前几年对深度学习及智能的期望并未达到人们及投资者的预期目标。今年年初，李开复曾指出：“2018年是AI泡沫破裂之年”。</p>
<p>从两年前人工智能热潮的爆发，到现在热潮逐渐退去，人们也越来越趋于冷静来认识人工智能，对其研究与应用逐渐趋于务实。笔者也希望人工智能能真正快速发展，让人类的生活变得越来越美好！</p>
<p>10.4.    深度学习与人工智能的关系</p>
<p>1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”（Artificial Intelligence， AI）的概念，梦想着用计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。人工智能研究领域范围很广，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等，如图27 （左）。</p>
<p>机器学习（Machine Learning, ML）是一种实现人工智能的方法。其基本做法是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。传统的机器学习算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习等。</p>
<p>深度学习（Deep Learning, DL）是一种实现机器学习的技术，是解决特征表达的一种学习过程。现在常指利用深度神经网络的深度学习。在很早就有人提过，但由于当时训练数据量不足、计算能力落后，因此深度学习的效果不尽如人意；直到近几年，深度学习才较好地实现了各种任务。事实上，除了基于深度神经网络的深度学习，还有其他形式（比如深度森林）的深度学习方法。</p>
<p>因此，机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术，而深度神经网络是实现深度学习的一种具体的实现工具，如图28（右）所示。简而言之，从范畴来讲，他们的关系如下：</p>
<p>深度神经网络 &lt; 深度学习 &lt; 机器学习 &lt; 人工智能</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image171.jpg" alt="img"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image172.jpg" alt="img"></p>
<p>图28. 左：人工智能的研究范畴；右：人工智能、机器学习和深度学习的关系。</p>
<p>（图片来自互联网）</p>
<p>但残酷的事实是，近几年流行的基于深度神经网络的深度学习仅仅是一个数据拟合器，并不存在着分析与理解等智能的能力。现有的技术和方法离真正的人工智能还非常遥远！</p>
<p><strong>十一、</strong>    <strong>三维几何数据</strong></p>
<p>在计算机图形学中，我们研究的对象是三维（3D）形状对象，即虚拟空间中的数字化的物理实体。表达3D对象的3D几何数据是继声音、图像、视频之后的第四代数字可视媒体（如图29），已逐渐具有越来越广泛的应用。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image173.jpg" alt="img"></p>
<p>图29. 四代可视媒体的发展：声音、图像、视频、3D几何。</p>
<p>11.1.    3D 数据的重要性</p>
<p>地球上的人及所有动物都有两只眼睛，每只眼睛类似于一台照相机，看到的是场景在视网膜（成像平面）上的投影图像，如图30（左）。两只眼睛位置不同，就产生对同一场景的视差，从而在大脑生成了场景的立体和3D信息（双目立体视觉原理），如图30（右）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image174.jpg" alt="img"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image175.png" alt="img"></p>
<p>图30. 左：单眼看到的是图像，为三维世界在成像平面上的投影影像；右：人类及动物的两只眼睛通过双眼的视差产生了对场景物体的立体和三维的信息。（图片来自于互联网）</p>
<p>因此，人类及动物的视觉感知系统是具有立体的特性。世界是三维的，3D模型提供了世界及所有物体的全方位、全息的表达，能提供比二维图像更丰富、更全面的空间信息（比如猎鹰能在几公里外判断草地上一只兔子的远近和方位，以便做出下一步的飞行行为）。而单眼（如果以前存在的话）动物则因无法分辨物体的远近，而无情地被自然法则所淘汰。</p>
<p>计算机视觉通过对图像的理解和分析已经能够解决非常多的任务（比如物体检测、分割、识别、检索等）。但在很多实际应用中，如果涉及到空间关系分析以及与3D环境和物体的理解、交互和创造，则光靠2D图像信息是不够的，此时必须依赖场景的3D信息，比如数字城市中的空间关系分析、机器人抓取3D物体、虚拟和增强现实、物联网数据的空间关联等，如图31所示。在这些应用中，仅仅靠图像信息是无法很快完成各项任务的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image176.jpg" alt="img"></p>
<p>图31. 三维数据在很多实际应用场景中是必不可少的信息。</p>
<p>11.2.    3D数据集</p>
<p>随着3D物体的扫描与采集手段的日益增多（特别是近几年快速发展的深度相机，比如Microsoft Kinect、Intel RealSense、Google Project Tango、Apple Primesense、Asus Xtion等）、未来可采集3D信息的移动终端（去年iPhoneX上已有深度相机）的普及、以及互联网上3D模型的创造与分享，3D数据（包括RGB-D数据）将日益丰富。</p>
<p>至今，已有许多研究机构构建并公布了一些3D数据集，比如：</p>
<p>ShapeNet数据集：包含了约300多万个3D模型（3000多类物体），每个模型有语义注释。这是当前最大的一个三维模型数据集；其子集ShapeNetCore具有一些手工的标注；</p>
<p>ModelNet数据集：包含了12多个3D模型（662类），包含两个子集Model10和ModelNet40；</p>
<p>ShapeGoogle数据集：包含了近600个非刚性3D模型，用于检索测试；</p>
<p>SHREC数据集：包含了上千个非刚性3D模型，包括不少人的模型和合成数据集；</p>
<p>Watch-N-Patch数据集：由Cornell大学和Stanford大学公布，主要是人体3D模型数据。</p>
<p>还有一些高校和研究机构公开了一些RGB-D数据集，包括Princeton大学（415个室内场景，990万个标记图像），纽约大学（464个室内场景），Cornell大学（24个办公室场景，28个居家室内场景），Washington大学（14个室内场景）等。</p>
<p>另外，瑞士苏黎世大学发布了一个使用LiDAR（激光3D扫描）扫描的40个办公室场景的高精度3D场景模型数据集。</p>
<p>11.3.    3D形状描述子</p>
<p>类似于2D图像，人们也需要对3D模型数据进行分类与识别、分割与建模、匹配与检索等。完成这些任务的关键就是如何定义和计算3D模型的形状描述子（Shape descriptors）或特征（Features）。</p>
<p>至今，人们已提出了各种基于全局和局部的形状几何属性的空间分布或统计信息的3D形状描述子（如图32所示）。这些形状描述子都是针对某种特定的应用或某些特定的3D模型类型而提出的，同时满足所有应用及所有模型类型的3D形状描述子还没有。在实际应用中，应根据物体类型和具体应用来选择合适的形状描述子（特征选择）。比如，笔者如下论文使用稀疏选择的方法来选择合适的形状描述子用于3D 形状分割。</p>
<p>[11-1] Ruizhen Hu, Lubin Fan, Ligang Liu. Co-Segmentation of 3D Shapes via Subspace Clustering. Computer Graphics Forum (Proc. Symposium on Geometry Processing), 31(5): 1703-1713, 2012.</p>
<p>11.4.    3D几何数据的深度学习的挑战</p>
<p>随着3D数据（包括单个模型以及场景模型）的日益流行和丰富，如何利用上述丰富的3D数据集来完成分析和处理3D形状数据，已成为研究工作者及产业工作者的关注焦点和研究热点。因此，3D形状的特征提取方法也逐步从上述的人工定义特征方法到基于深度学习的特征学习方法的发展。</p>
<p>相对于图像与视频处理，3D几何数据的深度学习方法在近几年才开始有了一些工作和进展，但仍面临着一些挑战。</p>
<p>(1)  3D数据集较小</p>
<p>相比于ImageNet等千万级数据量的2D图像数据集，现在的3D模型数据集的数量很少，仍未达到“大数据”的规模。另外，3D数据的标注也是较为困难的。</p>
<p>(2)  3D数据是非结构化的</p>
<p>图像和视频都是结构化数据，是线性结构的，可以表达为一个向量或矩阵。但3D模型数据是非结构化的（如图33所示），每个顶点的邻域数是非固定的，从拓扑上看是一个图（Graph），无法直接使用深度神经网络。</p>
<p>(3)  3D数据的复杂性</p>
<p>相对于图像和视频，3D模型还有其他一些复杂性，包括：</p>
<p>(a) 正向姿态性：图像基本都是正向拍摄的，但是3D模型的姿态可以是任意的，如何消除姿态的影响仍有挑战。为了方便，一般会预先将数据集中的模型统一正朝向；</p>
<p>(b) 表达不统一：图像就是一个矩阵结构，表达非常统一；但是3D模型有不同的表达方式，比如点云、网格、体素、隐函数、CSG树等（如图34），使得需要对不同表达的数据进行不同的处理；</p>
<p>(c) 数据不完整：由于采集设备的精度问题，有些3D数据（比如RGBD数据）中存在着大量的噪声和游离点；由于遮挡关系，有些3D数据不够完全，存在着空洞；还有些3D数据的采样密度不均匀，造成不同部位的信息不对称。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image177.jpg" alt="img"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image178.jpg" alt="img"></p>
<p>图32. 各种人工3D形状描述子。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image179.jpg" alt="img"></p>
<p>图33. 二维图像与三维网格的数据结构的区别。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image180.jpg" alt="img"></p>
<p>图34. 三维几何数据的不同表达。</p>
<p><strong>十二、</strong>    <strong>智能图形处理：三维数据的深度学习</strong></p>
<p>3D图形作为一种新型的可视媒体数据类型，同样可以利用人工智能和深度学习的方法来进行分析和处理，融合大数据、并行计算及人工智能技术的最新进展以提高计算机图形学算法及系统易用性及效率，称为智能图形处理。</p>
<p>近几年，已有越来越多的研究工作将机器学习和深度学习的方法应用于3D数据。本节仅就3D形状描述子方面做一个简略的介绍。</p>
<p>应用深度学习来自动抽取3D形状特征主要有以下3种方法：第一种，基于传统的人工特征（维数一致了）来进行更抽象的形状特征的学习和抽取；这种方法是非端到端的；第二种，将3D数据转化为规整结构数据（欧氏区域），然后再应用深度学习方法抽取形状特征，称为显式方法；第三种，将深度神经网络改造成能够处理非欧氏区域数据，称为隐式方法。下面分别从方法论上进行简单介绍。</p>
<p>12.1.    基于人工特征的特征学习方法</p>
<p>由于传统卷积无法直接作用于3D形状，早期的方法避开了直接以3D形状作为输入的训练，而是先从3D模型中提取一些人工特征（称为低级特征，如图32），再把这些特征输入到神经网络中进行学习，获得新的抽象特征（称为高级特征），如图35所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image181.jpg" alt="img"></p>
<p>图35. 基于人工特征的特征学习方法。</p>
<p>[12-1] Zhige Xie, Kai Xu, Ligang Liu, Yueshan Xiong. 3D Shape Segmentation and Labeling via Extreme Learning Machine. Computer Graphics Forum (Proc. SGP), 33(5): 85-95, 2014.</p>
<p>[12-2] Kan Guo, Dongqing Zou, and Xiaowu Chen. 3D mesh labeling via deep convolutional neural networks. ACM Transactions on Graphics, 35, 1, 2015.</p>
<p>[12-3] Zhenyu Shu, Chengwu Qi, Shiqing Xin, Chao Hu, Li Wang, Yu Zhang, Ligang Liu. Unsupervised 3D Shape Segmentation and Co-segmentation via Deep Learning. Computer Aided Geometric Design (Proc. GMP), 43: 39-52, 2016.</p>
<p>人工定义的特征很大程度上取决于人的经验。因此，基于人工特征的特征学习方法并不是端到端的深度学习方法。另外，各种人工特征向量的排列也会改变特征矩阵及其卷积后的结果，对结果产生如何的影响也不清楚。</p>
<p>12.2.    非本征方法</p>
<p>这种方法是将3D数据进行合适的变换，将其变换到一个欧氏空间的规则区域，以便适用于深度神经网络。此类方法改变了3D网格模型的形态，本质上也改变了数据分布空间，是为了适应传统CNN或MLP对拓扑的要求而提出的折衷解决方法，因此成为基于欧氏空间的学习方法或非本征方法，如图36所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image182.jpg" alt="img"></p>
<p>图36. 非本征深度学习方法。</p>
<p>变换的区域和方法主要有以下几种。</p>
<p>(1)  图像区域。最简单的方法就是将3D模型数据投影到平面形成图像。</p>
<p>(a)  多视图图像：将3D模型往多个视角方向投影生成多幅图像，比如[12-4, 12-5]；</p>
<p>(b)  全景图像：将3D模型往平面投影生成一幅全景图，比如[12-6]；</p>
<p>(c)  参数化图像：将3D模型的2D参数化看成为一幅图像，比如[12-7, 12-8]。</p>
<p>(2)  体素区域。</p>
<p>(a)  类比于图像中的像素，将3D形状看作三维体素网格的0，1分布，比如[12-9, 12-10， 12-11]。由于体素的个数是三次方增长，因此，体素的分辨率无法太大，一般连128x128x128的分辨率做起来都很困难。</p>
<p>(b)  只将模型表面用体素来表达，而省去内部体素的存储，并且利用八叉树的形式来存储，可以有效地提高体素的分辨率及模型的表达精度，比如[12-12]。</p>
<p>(3)  基元组合。三维形状还可以表示成一些基本单元的组合，如矩形块、圆柱、圆锥、球等，然后利用网络来学习这些基本体块的参数，比如[12-13, 12-14]。</p>
<p>(4)  点云。直接将3D数据看成点的集合。将每个点看作一个神经元节点，节点包含点的坐标或法向等信息，然后利用深度神经网络提取点的特征。这里要克服点及点邻域的顺序无关性等。比如[12-14, 12-15, 12-16, 12-18]。</p>
<p>[12-4] Su, H. and S. Maji, et al. (2015). Multi-view convolutional neural networks for 3D shape recognition. ICCV.</p>
<p>[12-5] Xie, Z. and K. Xu, et al. (2015). Projective Feature Learning for 3D Shapes with Multi‐View Depth Images. CGF.</p>
<p> [12-6] Shi, B. and S. Bai, et al. (2015). Deeppano: Deep panoramic representation for 3‐D shape recognition. IEEE Signal Processing Letters.</p>
<p> [12-7] Sinha, A., Bai, J., Ramani, K., et al. (2016). Deep learning 3D shape surfaces using geometry images. ECCV.</p>
<p>[12-8] Maron, H. and Galun, M. (2017). Convolutional Neural Networks on Surfaces via Seamless Toric Covers. SIGGRAPH.</p>
<p>[12-9] Qi, C. R. and H. Su, et al. (2016). Volumetric and multi-view cnns for object classification on 3D data. CVPR.</p>
<p>[12-10] Brock, A. and T. Lim, et al. (2016). Generative and discriminative voxel modeling with convolutional neural networks. NIPS.</p>
<p>[12-11] Wu, Z., and Song, S. et al. (2015). 3d shapenets: a deep representation for volumetric shapes. CVPR.</p>
<p>[12-12] Wang, P. S. and Liu, Y. et al. (2017). O-cnn: octree-based convolutional neural networks for 3d shape analysis. TOG.</p>
<p>[12-13] J. Li, K. Xu, et al. Grass: Generative recrusive autoencoders for shape structures. ACM Trans. on Graph., 36(4), 2017.</p>
<p>[12-14] S. Tulsiani et al. Learning shape abstraction by assembling volumetric primitives. CVPR, 2017.</p>
<p>[12-15] Garcia-Garcia, A. and Gomez-Donoso, F. et al. (2016). PointNet: A 3D Convolutional Neural Network for real-time object class recognition. IJCNN.</p>
<p>[12-16] Qi, C. R. and H. Su, et al. (2017). PointNet: Deep learning on point sets for 3D classification and segmentation. CVPR.</p>
<p>[12-17] Qi, C. R. and Yi, L. et al. (2017). PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. CVPR.</p>
<p>[12-18] Yangyan Li, Rui Bu, Mingchao Sun, and Baoquan Chen (2018). PointCNN. arXiv:1801.07791.</p>
<p>12.3.    本征方法</p>
<p>本征方法直接将三维形状看成二维流形（Manifold）或由点组成的图（Graph），顶点之间的距离不再是欧氏距离，然后直接将卷积定义在这样的数据结构上，称为非欧氏空间学习的方法或本征方法，如图37。两种典型的网络为测地卷积网络（Geodesic CNN, GCNN）和图卷积网络（Graph-based CNN），如图38，可参考[12-19, 12-20, 12-21, 12-22]。</p>
<p>[12-19] Masci, J. and Boscaini, D. et al. (2015). Geodesic convolutional neural networks on Riemannian manifolds. ICCV.</p>
<p>[12-20] Monti, F. and Boscaini, D., Masci. (2016). Geometric deep learning on graphs and manifolds using mixture model CNNs. arXiv preprint arXiv:1611.08402.</p>
<p>[12-21] Boscaini, D. and Masci, J. et al. (2016). Learning shape correspondence with anisotropic convolutional neural networks. NIPS.</p>
<p>[12-22] Yi, L. and Su, H. et al. (2017). Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation. CVPR.</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image183.jpg" alt="img"></p>
<p>图37. 本征深度学习方法。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image184.jpg" alt="img">  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image185.jpg" alt="img"></p>
<p>图38. 本征深度学习的具体方法。左：测地卷积；右：图卷积网络。</p>
<p>12.4.    端到端的生成模型</p>
<p>在上面的工作中，深度学习用于解决3D形状的识别、分割、匹配和对应等问题。近年来，越来越多的工作致力于3D 模型的构建和生成（大部分都是使用生成-对抗网络GAN）。由于可以自动生成大量的3D模型，对于扩充3D模型数据集具有重要的意义，非常值得大家关注。</p>
<p>这里简略介绍一下端到端的3D模型生成的一些工作。所谓端到端的生成模型，就是输入是简单易获得的，输出是三维模型，中间无需其他操作。例如，可以从输入的一张照片输出三维体素模型[12-23, 12-24]、点云模型[12-25]、或者网格模型及其参数化[12-26]；[12-27]利用级联的图卷积网络由粗到精逐渐细化网格模型；[12-28]由手绘草图生成参数化的夸张人脸模型。</p>
<p>[12-23] J. Wu et al. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. Advances in Neural Information Processing Systems, 82-90, 2016.</p>
<p>[12-24] J. Li et al. Grass: Generative recursive autoencoders for shape structures. Siggraph 2017.</p>
<p>[12-25] H. Fan et al. A point set generation network for 3D object reconstruction from a single image. CVPR 2017.</p>
<p>[12-26] T. Groueix et al. AltasNet: A papier approach to learning 3D surface generation. arXiv: 1802.05384, 2018.</p>
<p>[12-27] N. Wang et al. Pixel2Mesh: Generating 3D mesh models form single RGB iamges. arXiv: 1804.01654.</p>
<p>[12-28] X. Han et al. DeepSketch2Face: A deep learning based sketching systems for 3D face and caricature modeling. Siggraph 2017.</p>
<p>【后记】</p>
<p>此文分享了笔者对基于深度神经网络的深度学习的理解，从逼近论的角度深入浅出地理解了深度学习的原理，了解了整个调试深度网络的“炼丹”过程，是个需要丰富经验的“技术活”。机器学习还有很多其他重要的方法，比如非监督学习和强化学习等，笔者就不一一进行介绍了。</p>
<p>从本文的分析可知，基于深度神经网络的深度学习背后的运行原理主要基于数据驱动的函数拟合，采用的是“经验主义”的实用方法（比如数学中的最小二乘方法及统计学中的统计回归方法等）；它离真正的人工智能，即能通过图灵测试（Turing Testing）的智能机器还很远。从数学本质上来看，现在基于数据的深度学习方法实质上只有“记忆”能力，并没有对问题的“理解”能力，更没有 人们所期待的“人类智能”。</p>
<p>从研究的方法论来看，对于要研究的对象（信息或数据），如果在不同的空间（角度）来观察会有对该对象有不同的理解和认知（比如信号处理中，在时域和频域的变换下的Fourier分析），即对象在该空间中所表现的“特征”。因此，<strong>在不同的空间进行变换</strong>就成为关键的步骤之一。在以往的研究中，需要研究者对对象有深刻的理解来对信息进行特定的变换（即在特定的基函数的变换下）。而深度神经网络实现了端到端的变换，通过多层（深度）复合的变换（多次变换）自动提取了在不同空间的“特征”，即根据数据本身自适应“学习”基函数。这是深度神经网络的巨大优势之一。另外，深度神经网络的另一个巨大优势就是可以通过数量众多的小函数（激活函数）的线性变换及复合来逼近非常复杂的函数，解决了人工设计基函数的困难。因此，基于深度神经网络的深度学习具有非常广泛的实际应用。另外值得一提的是，从Fourier变换到小波变换的发展来看，可以将神经网络中的激活函数变为特殊的局部基函数，使得比现有的全局激活函数适用于更广泛的函数（比如瞬时信号函数），将来再抽时间来详细分析和阐述。</p>
<p>从计算数学领域来看，<strong>神经网络提供了可以构建任意维度之间的映射和函数</strong>，这是传统逼近论所无法做到的。这使得人们研究的数据维度和数据类型极大的丰富了，而不只是对1维和2维等低维数据进行分析和处理。这将是一个非常重要的方法论，将对科学领域的研究方法、手段和进展也具有极大的促进和推动作用。有理由相信，深度学习方法（或人工智能方法）将会被当作计算数学、应用科学的计算工具或科研工具，催生新的科研范式，被越来越多地用来解决一些计算数学和其他科学领域中的问题。</p>
<p>从方法论来看，也可以从<strong>统计学的角度</strong>来看深度神经网络，将深度神经网络来拟合数据的分布（分布函数），比如生成网络和GAN等，也是近期研究的热点。在本文中对此方法论没有进行展开。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image186.jpg" alt="img"></p>
<p>图39. Gartner 2018人工智能技术成熟度曲线。（图片来源于互联网）</p>
<p>五年前（2013年4月），《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术（Breakthrough Technology）之首。经过前几年的高速发展，根据Gartner在7月刚发布的2018人工智能技术成熟度曲线（如图39），深度神经网络及深度学习已从前几年的爬坡阶段到达顶部。未来，相信人们将会更冷静地来看待及发展相关的技术和应用。</p>
<p>最近，有人提出“可微编程（Differentiable Programming）”的概念，就是将神经网络当成一种语言（而不是一个简单的机器学习的方法），来描述我们客观世界的规律。甚至Yann LeCun曾在Facebook的文章中说道：“Deep Learning Is Dead. Long Live Differentiable Programming!”（深度学习已死，可微编程永生） 这个概念的提出又将神经网络提高了一个层次。</p>
<p>本文完稿核对时，笔者发现文中有些数学符号（比如<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.files/image187.png" alt="img">等）存在着重复定义与使用，但根据上下文并不影响阅读和理解，因此笔者就不花时间去修正了，在此说明下。</p>
<p>再次声明下，笔者的主要研究领域为<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu/Resources/CG/What_is_CG.htm">计算机图形学</a>，而非人工智能领域，因此本文仅仅为笔者从外行的角度对基于深度神经网络的深度学习的粗浅理解，而非人工智能领域对深度神经网络和深度学习的权威解释。笔者对其中的有些内容的理解是有限的，甚至是有误的。因此，该文仅供读者参考，不作为专业资料！如有不当之处，还请读者指正！</p>
<p>【致谢】</p>
<p>笔者在学习深度神经网络及深度学习的过程中，阅读了许多相关的书籍和论文（比如周志华老师的《机器学习》一书和李宏毅老师的课件等）以及微信订阅号（比如《SigAI》、《人工智能学家》、《深度学习大讲堂》、《老顾谈几何》等），在此表示感谢。也感谢舒振宇、徐凯、韩晓光、沈小勇、胡瑞珍等同行、以及笔者实验室的同事（张举勇、傅孝明、杨周旺等）和学生（陈明佳、方清、杜冬、石磊、欧阳文清、刘中远等），与他们的交流和讨论也让笔者受益匪浅。</p>
<p>最后，希望您能从本文受益。祝您健康、快乐、成功！</p>
<p>刘利刚</p>
<p>中国科学技术大学图形与几何计算实验室(<a target="_blank" rel="noopener" href="http://gcl.ustc.edu.cn/">http://gcl.ustc.edu.cn</a>)</p>
<p>个人主页：<a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~lgliu">http://staff.ustc.edu.cn/~lgliu</a></p>
<p>电子邮箱：<a href="mailto:lgliu@ustc.edu.cn">lgliu@ustc.edu.cn</a></p>
<p>2018年8月8日</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">ytkz</div><div class="post-copyright__author_desc">Some personal notes</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/')">Find everything you need</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=Find everything you need&amp;url=http://ytkz11@.github.io/2024/12/10/45-fen-zhong-li-jie-shen-du-shen-jing-wang-luo-he-shen-du-xue-xi/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://ytkz11@.github.io" target="_blank">Find everything you need</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507111500105.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/04/90-xing-dai-ma-zhi-zuo-yao-gan-bian-hua-jian-ce-yang-ben/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202412041038806.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">90行代码，由遥感影像制作遥感变化检测样本的教程</div></div></a></div><div class="next-post pull-right"><a href="/2025/01/01/zhi-zuo-yao-gan-biao-qian-yang-ben-de-guo-cheng/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202501021659183.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">制作遥感标签样本的一般流程</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://raw.githubusercontent.com/ytkz11/picture/e0561f27bb351a8dc757d4f5b1d74a1ab5ef4e42/imgs202411281530223.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/xue-xi-rasterio-huan-shi-gdal/" title="学习Rasterio还是GDAL？"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507161641974.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="学习Rasterio还是GDAL？"/></a><div class="content"><a class="title" href="/2025/07/16/xue-xi-rasterio-huan-shi-gdal/" title="学习Rasterio还是GDAL？">学习Rasterio还是GDAL？</a><time datetime="2025-07-16T12:20:10.000Z" title="发表于 2025-07-16 20:20:10">2025-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/11/gao-bie-gdal-translate-ying-jie-gdal-tong-yi-ming-ling-xing-xin-shi-dai/" title="告别 gdal_translate：迎接 GDAL 统一命令行新时代"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507111500105.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="告别 gdal_translate：迎接 GDAL 统一命令行新时代"/></a><div class="content"><a class="title" href="/2025/07/11/gao-bie-gdal-translate-ying-jie-gdal-tong-yi-ming-ling-xing-xin-shi-dai/" title="告别 gdal_translate：迎接 GDAL 统一命令行新时代">告别 gdal_translate：迎接 GDAL 统一命令行新时代</a><time datetime="2025-07-11T12:20:10.000Z" title="发表于 2025-07-11 20:20:10">2025-07-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/11/jian-yao-jie-shao-gdal-de-jia-gou-guan-jian-zu-jian-he-gong-neng/" title="简要介绍 GDAL 的架构、关键组件和功能"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507111438678.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="简要介绍 GDAL 的架构、关键组件和功能"/></a><div class="content"><a class="title" href="/2025/07/11/jian-yao-jie-shao-gdal-de-jia-gou-guan-jian-zu-jian-he-gong-neng/" title="简要介绍 GDAL 的架构、关键组件和功能">简要介绍 GDAL 的架构、关键组件和功能</a><time datetime="2025-07-11T12:20:10.000Z" title="发表于 2025-07-11 20:20:10">2025-07-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/10/gdal-ogr-jian-dan-de-shi-yong-jiao-cheng/" title="GDAL_OGR简单的使用教程"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507161641982.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GDAL_OGR简单的使用教程"/></a><div class="content"><a class="title" href="/2025/07/10/gdal-ogr-jian-dan-de-shi-yong-jiao-cheng/" title="GDAL_OGR简单的使用教程">GDAL_OGR简单的使用教程</a><time datetime="2025-07-10T12:20:10.000Z" title="发表于 2025-07-10 20:20:10">2025-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/10/yao-gan-dao-di-shi-ge-sha/" title="遥感到底是个啥"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/ytkz11/picture/imgs202507101625855.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="遥感到底是个啥"/></a><div class="content"><a class="title" href="/2025/07/10/yao-gan-dao-di-shi-ge-sha/" title="遥感到底是个啥">遥感到底是个啥</a><time datetime="2025-07-10T12:20:10.000Z" title="发表于 2025-07-10 20:20:10">2025-07-10</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="ytkz" target="_blank">ytkz</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">317</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">52</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">80</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/GDAL/" style="font-size: 0.88rem;">GDAL<sup>4</sup></a><a href="/tags/GEE/" style="font-size: 0.88rem;">GEE<sup>1</sup></a><a href="/tags/GIS/" style="font-size: 0.88rem;">GIS<sup>8</sup></a><a href="/tags/IDL/" style="font-size: 0.88rem;">IDL<sup>1</sup></a><a href="/tags/JavaScript/" style="font-size: 0.88rem;">JavaScript<sup>1</sup></a><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>11</sup></a><a href="/tags/SAR/" style="font-size: 0.88rem;">SAR<sup>2</sup></a><a href="/tags/Sentinel5/" style="font-size: 0.88rem;">Sentinel5<sup>3</sup></a><a href="/tags/arcgis/" style="font-size: 0.88rem;">arcgis<sup>1</sup></a><a href="/tags/cpp/" style="font-size: 0.88rem;">cpp<sup>2</sup></a><a href="/tags/deep-learn/" style="font-size: 0.88rem;">deep_learn<sup>1</sup></a><a href="/tags/ffmpeg/" style="font-size: 0.88rem;">ffmpeg<sup>1</sup></a><a href="/tags/gdal/" style="font-size: 0.88rem;">gdal<sup>5</sup></a><a href="/tags/gis/" style="font-size: 0.88rem;">gis<sup>5</sup></a><a href="/tags/golang/" style="font-size: 0.88rem;">golang<sup>1</sup></a><a href="/tags/himawari8/" style="font-size: 0.88rem;">himawari8<sup>1</sup></a><a href="/tags/kotlin/" style="font-size: 0.88rem;">kotlin<sup>3</sup></a><a href="/tags/leetcode/" style="font-size: 0.88rem;">leetcode<sup>1</sup></a><a href="/tags/linux/" style="font-size: 0.88rem;">linux<sup>6</sup></a><a href="/tags/matlab/" style="font-size: 0.88rem;">matlab<sup>4</sup></a><a href="/tags/pyside6/" style="font-size: 0.88rem;">pyside6<sup>2</sup></a><a href="/tags/python/" style="font-size: 0.88rem;">python<sup>47</sup></a><a href="/tags/rust/" style="font-size: 0.88rem;">rust<sup>1</sup></a><a href="/tags/sar/" style="font-size: 0.88rem;">sar<sup>5</sup></a><a href="/tags/shp/" style="font-size: 0.88rem;">shp<sup>1</sup></a><a href="/tags/vs/" style="font-size: 0.88rem;">vs<sup>1</sup></a><a href="/tags/%E4%B8%8B%E8%BD%BD/" style="font-size: 0.88rem;">下载<sup>1</sup></a><a href="/tags/%E5%8C%80%E8%89%B2/" style="font-size: 0.88rem;">匀色<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">工具<sup>2</sup></a><a href="/tags/%E6%8A%80%E6%9C%AF/" style="font-size: 0.88rem;">技术<sup>3</sup></a><a href="/tags/%E6%A1%88%E4%BB%B6/" style="font-size: 0.88rem;">案件<sup>1</sup></a><a href="/tags/%E6%B3%A8%E5%86%8C%E6%B5%8B%E7%BB%98%E5%B8%88/" style="font-size: 0.88rem;">注册测绘师<sup>13</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">深度学习<sup>15</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 0.88rem;">爬虫<sup>1</sup></a><a href="/tags/%E7%9F%A2%E9%87%8F/" style="font-size: 0.88rem;">矢量<sup>26</sup></a><a href="/tags/%E7%BB%98%E5%9B%BE/" style="font-size: 0.88rem;">绘图<sup>1</sup></a><a href="/tags/%E7%BC%96%E7%A8%8B/" style="font-size: 0.88rem;">编程<sup>19</sup></a><a href="/tags/%E9%81%A5%E6%84%9F/" style="font-size: 0.88rem;">遥感<sup>184</sup></a><a href="/tags/%E9%81%A5%E6%84%9F-%E7%9F%A2%E9%87%8F/" style="font-size: 0.88rem;">遥感,矢量<sup>1</sup></a><a href="/tags/%E9%97%B2%E8%81%8A/" style="font-size: 0.88rem;">闲聊<sup>2</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 ytkz 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"react":{"opacity":0.7}});</script></body></html>